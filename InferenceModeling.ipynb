{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np      \n",
    "import math\n",
    "import argparse\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "******  MI300 benchmarks (roofline) ****************\n",
      "****************************************************\n",
      "acheiveable memory bw (Gbytes/s) 4.34176\n",
      "achieveable compute (tera) flops = 933.888\n",
      "kv_cache capacity (Gbytes) =5.078125\n",
      " maximum batch_size we could use (per GPU) = 56.0\n",
      "\n",
      "Using model-size as flops (Approximate)\n",
      "compute_time 3.7\n",
      "communcation time 1.9\n",
      "1 - token generation (ms): 3.7 \n",
      "\n",
      "Using flops-calculator:\n",
      "1token generation (ms) 1.9709 \n",
      "\n",
      "151190901882880\n",
      "Using flops-calculator:\n",
      "compute_time 47.6 (ms)\n",
      "communication time 34.2 (ms)\n",
      "2048-token generation time = 47.62 (ms)\n"
     ]
    }
   ],
   "source": [
    "#LLM Inference roofline modeling for AMD GPU Mi300\n",
    "#infernce has two passes. prefill + token generation\n",
    "# time taken = prefill + token_gen\n",
    "# prefill (prompt processing) = run through once forward pass for entire prompt sequence length\n",
    "# decode = generate out token length for each step\n",
    "# for large batch-size, both prefill and token is bound by linear layer flops\n",
    "# usually inference has smaller batch-size , hence token generation phase dominated\n",
    "# by bandwidth bound linear layer.\n",
    "\n",
    "#llama_65B model configurtion  parameters\n",
    "max_seq_len = 2048\n",
    "model_dim = 8192\n",
    "hidden_dim = 22016\n",
    "num_layers = 80\n",
    "num_heads = 64\n",
    "out_tokens = 32\n",
    "vocab_size = 32000\n",
    "model_size = 65  # 1e9 Billion\n",
    "\n",
    "def qkv_proj(seqlen:int =1):\n",
    "    return 2*3*model_dim*seqlen\n",
    "\n",
    "def out_proj(seqlen:int =1):\n",
    "    return 2*model_dim*model_dim*seqlen\n",
    "\n",
    "def mlp(seqlen:int = 1):\n",
    "    return 4*model_dim*hidden_dim*seqlen\n",
    "\n",
    "def fa(seqlen:int = 1):\n",
    "    return (2*2*seqlen*seqlen*model_dim)\n",
    "\n",
    "#forward pass flops calculation\n",
    "qkv_flops = qkv_proj(max_seq_len)\n",
    "out_flops = out_proj(max_seq_len)\n",
    "mlp_flops = mlp(max_seq_len)\n",
    "#use flash attention for  self-attention layer in both prefill and token generation   \n",
    "fa_flops  = fa(max_seq_len)\n",
    "#total_flops = num_layers * (qkv_proj+out_proj+mlp+fa_flops)\n",
    "\n",
    "#required flops for 65B model \n",
    "total_flops = num_layers * (qkv_flops+out_flops+mlp_flops+fa_flops)\n",
    "print(\"****************************************************\")\n",
    "print(\"******  MI300 benchmarks (roofline) ****************\")\n",
    "print(\"****************************************************\")\n",
    "#mi300 configuraiton fp16 precision\n",
    "memory_bw = 5.3 * 64*128/8  # (64 bits/ channel 128 channel 5.3 bit rate)\n",
    "achieved_bw = .8 * memory_bw / 1000\n",
    "print(f\"acheiveable memory bw (Gbytes/s) {achieved_bw}\")\n",
    "compute_flops = 304*2048*1.5/1000    # core frequency = 1.5Ghz\n",
    "gemm_eff = 0.85\n",
    "print(f\"achieveable compute (tera) flops = {compute_flops}\")\n",
    "#acheived memory bw (Gbytes/s) 4341.76\n",
    "#achieveable compute (tera) flops = 933.888\n",
    "\n",
    "#capacity \n",
    "MI300_hbm_capacity = 192   #GBytes/GPU\n",
    "#capcity required by model (weights + KV cache)\n",
    "#kv cache - token generation phase require past attention KV values for every\n",
    "#step of the phase (source GPT2- paper), instead regenerating KV tensors, store them \n",
    "# in cache / hbm and read back for every step.\n",
    "\n",
    "model_capacity = 65*2   # fp16 precision\n",
    "kv_cache_capacity = 2*2*model_dim*num_layers*(max_seq_len+out_tokens)/1024/1024/1024   #per max-seq-len\n",
    "print(f\"kv_cache capacity (Gbytes) ={kv_cache_capacity}\")\n",
    "#kv_cache capacity (Gbytes) =5.078125\n",
    "\n",
    "batch_size_max = np.floor((MI300_hbm_capacity-(model_capacity+kv_cache_capacity)))\n",
    "print(f\" maximum batch_size we could use (per GPU) = {batch_size_max}\")\n",
    "#maximum batch_size we could use (per GPU) = 56.0\n",
    "print(\"\")\n",
    "\n",
    "#using 1 GPU , we could fit 65B model and still do get decent  performance \n",
    "# with ~50 batch-size.\n",
    "# there are two reasons, we want to go upto 8 GPu(s) are,\n",
    "# 1. higher batch-size to make problem size as compute bound\n",
    "# 2. apply (tensor,pipeline, data) parallelism to inferenece to get higher thoughput\n",
    "\n",
    "#parallelism trade-offs\n",
    "# sharding layer,weights, data across n GPUs would help us get higher througput but it also\n",
    "# introduces new parameters that needs to be part of calculations namely\n",
    "# communication payload between GPU(S), reduction , synchronization, gather and communication latency\n",
    "# \n",
    "\n",
    "# pipeline parallelism\n",
    "# each GPU is mapped to 'n' layers (total_layers/ pipeline rank devices) \n",
    "# each needs to send and recv inputs from neighbur devices through communication\n",
    "# network. \n",
    "\n",
    "# (MLP) tensor parallelism\n",
    "# weights of linear layer are split in the middle and mapped to GPU devices\n",
    "# after the completion of linear layer, all reduction op must be performed.\n",
    "# MLP layer split among 'n' devices, after second layer all devices \n",
    "# communicate hidden_dim parameters for reducation and scatter back.\n",
    "\n",
    "# attention layer parallelism\n",
    "# heads mapped to different devices, all devices communciate model_dim/N parameters\n",
    "\n",
    "\n",
    "#Communication perameters\n",
    "# payload (bytes xfered between devices)\n",
    "num_devices = 8\n",
    "payload_size = model_dim * (num_devices-1) / num_devices   #per token\n",
    "communication_latency = 12   # micro-seconds \n",
    "communication_bw = 448       # GBytes/s\n",
    "communication_eff = .7       # based on communication primitives micro-kernel\n",
    "\n",
    "#Batch_size =16 (latency bound case)\n",
    "# model performance is bounded by memory bw and communcation latency\n",
    "\n",
    "#token_genearation =- 65B model batch_size=16\n",
    "seq_len = 1\n",
    "kvcache_read = (num_layers*model_dim*max_seq_len*2*2 / 1e9) / (num_devices *achieved_bw)\n",
    "\n",
    "compute_time = np.round(2*model_size*1e9*seq_len / (num_devices *achieved_bw*1e12)  * 1e3,1)   # convert to ms\n",
    "communication_time = np.round(2*num_layers*communication_latency / 1e3,1)      # convert to ms\n",
    "\n",
    "#print(f\"kvache_read_time {kvcache_read}\")\n",
    "print(\"Using model-size as flops (Approximate)\")\n",
    "print(f\"compute_time {compute_time}\")\n",
    "print(f\"communcation time {communication_time}\")\n",
    "#missing embedding , unembedding and actation layers \n",
    "\n",
    "#ignoring kvcache read time taken per token \n",
    "print(f\"1 - token generation (ms): {max(compute_time,communication_time)} \")\n",
    "\n",
    "#using flops calculator\n",
    "qkv_flops = qkv_proj(seq_len)\n",
    "out_flops = out_proj(seq_len)\n",
    "mlp_flops = mlp(seq_len)\n",
    "#use flash attention for  self-attention layer in both prefill and token generation   \n",
    "fa_flops  = fa(seq_len)\n",
    "total_flops = num_layers * (qkv_flops+out_flops+mlp_flops+fa_flops)\n",
    "#print(num_layers*24*model_dim*model_dim)\n",
    "#print(f\"total_flops {total_flops}\")\n",
    "print(\"\")\n",
    "print(\"Using flops-calculator:\")\n",
    "compute_time = np.round((total_flops*seq_len)/(num_devices *achieved_bw*1e12)*1e3,4)\n",
    "print(f\"1token generation (ms) {compute_time} \")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "#large_batcsize = 512 tokens\n",
    "seq_len = 2048\n",
    "qkv_flops = qkv_proj(seq_len)\n",
    "out_flops = out_proj(seq_len)\n",
    "mlp_flops = mlp(seq_len)\n",
    "#use flash attention for  self-attention layer in both prefill and token generation   \n",
    "fa_flops  = fa(seq_len)\n",
    "total_flops = num_layers * (qkv_flops+out_flops+mlp_flops+fa_flops)\n",
    "#total_flops = total_flops/(1e9)\n",
    "total_flops = total_flops\n",
    "print(total_flops)\n",
    "total_flops = total_flops/1000/1000/1000\n",
    "compute_time = np.round(2*total_flops *1e9 / (num_devices*compute_flops*1e12*gemm_eff) * 1e3,2)\n",
    "communication_time = np.round((seq_len * 2 * 4 * num_layers * model_dim) / (1e9*communication_bw * communication_eff) *1e3,1)\n",
    "print(\"Using flops-calculator:\")\n",
    "print(f\"compute_time {np.round(compute_time,1)} (ms)\")\n",
    "print(f\"communication time {np.round(communication_time,1)} (ms)\")\n",
    "print(f\"{seq_len}-token generation time = {max(compute_time,communication_time)} (ms)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"Gpt3_175B\"  : {\"microbatch_size\": 1, \"batch_size\": 1, \"max_seq_len\": 2048, \"vocab_size\": 64000, \"model_dim\": 12288, \"hidden_dim\": 49152, \"num_layers\" : 96, \"attn_heads\" : 96, \"out_tokens\": 32},\n",
    "    \"llama_65B\"  : {\"microbatch_size\": 1, \"batch_size\": 1, \"max_seq_len\": 2048,\"vocab_size\": 32000, \"model_dim\": 8192, \"hidden_dim\": 22016, \"num_layers\" : 80, \"attn_heads\" : 64, \"out_tokens\": 32},\n",
    "}\n",
    "system_config = {\n",
    "    \"mi300\" : { 'hbm_cap': 192, 'lds_cap': 64,\n",
    "               'num_cus': 304,\n",
    "               'fp16_rate' : 2048, 'fmax' : 1.5 , 'l2_rdbw' : 16384,\n",
    "               'l2_wrbw': 8192, 'hbm_bw' : 6.4*8192, 'lds_bw' : 128, \n",
    "               'fma_rate' : 256, 'comm_bw' : 448 , 'comm_eff' : .7, 'comm_latency' : 12}\n",
    "}\n",
    "\n",
    "ProbMem = dict()\n",
    "ProbMem['KVCacheGpu'] = 0.5\n",
    "ProbMem['WeightsGpu'] = 0.5\n",
    "ProbMem['ActGpu'] = 0.5\n",
    "ProbMem['KVCacheCpu'] = 0.2\n",
    "ProbMem['WeightsCpu'] = 0.2\n",
    "ProbMem['ActCpu'] = 0.2\n",
    "ProbMem['KVCacheDisk'] = 0.3\n",
    "ProbMem['WeightsDisk'] = 0.3\n",
    "ProbMem['ActDisk'] = 0.3\n",
    "\n",
    "launch_setup_eff_loss = 5 \n",
    "NUM_GPUS = 8\n",
    "Bpe = 2   #fp16\n",
    "stream_bw_attainment = .8  #efficiency\n",
    "\n",
    "class MODELCOMPONENTS(Enum):\n",
    "    Weights = \"weights\"\n",
    "    Activation = \"activation\"\n",
    "    KVcache  = \"KVcache\"\n",
    "    MOES = \"Moes\"\n",
    "    \n",
    "class MODELPHASE(Enum):\n",
    "    PREFILL = 0\n",
    "    TOKEN = 1    \n",
    "    \n",
    "class MODELPIPELINE(Enum):\n",
    "    INFERENCE = 0\n",
    "    TRAINING = 1\n",
    "\n",
    "\n",
    "tile_sizes = [(16,16),(16,32),(32,16),(32,32),(32,64),(64,32),(64,64),(64,128),(128,64),(128,128),(256,256),(256,128)]\n",
    "tilek_sizes = [1024,512,256,128,64,32]\n",
    "\n",
    "#memory_bw formula\n",
    "def membw_acheived():\n",
    "  membw_rate = system_config['mi300']['hbm_bw']\n",
    "  eff = 0.65  # realistic efficiency that can be achieved\n",
    "  return membw_rate*eff/8\n",
    "\n",
    "def membound_gemm(M: int, N:int, K:int, gpu_name: str = \"mi300\"):\n",
    "    #check size\n",
    "    # convert size into per CU buffer \n",
    "    # using FMA ops vs MFMA ops\n",
    "    # we need ~150 CU(s) to sustain HBM BW. \n",
    "    _config = system_config[gpu_name]\n",
    "    number_of_elements = N*K if M<16 else M*K\n",
    "    gemm_m = N if M<16 else M\n",
    "    num_tiles_per_cu = gemm_m/16/_config['num_cus']\n",
    "    # do i need splitK?\n",
    "    if num_tiles_per_cu <=1:\n",
    "        split_k = 4\n",
    "    elif num_tiles_per_cu <=2:\n",
    "        split_k = 2\n",
    "    else:\n",
    "        split_k = 1\n",
    "    number_of_elements = number_of_elements/1024/1024/1024\n",
    "    time_duration = number_of_elements*2/(1e6*membw_acheived()*.85)\n",
    "    return time_duration\n",
    "\n",
    "def gemm_l2_bw(gemm_k: int, tile_size : tuple = (128,128), bpe:int =2, alu_rate: int = 2048):\n",
    "    num_flops = tile_size[0]*tile_size[1]*gemm_k*2\n",
    "    bytes_to_read = bpe*(tile_size[0]*gemm_k + tile_size[1]*gemm_k)\n",
    "    bytes_to_write = bpe*(tile_size[0]*tile_size[1])\n",
    "    l2_bw_need =  (bytes_to_read + bytes_to_write)/(num_flops//alu_rate)\n",
    "    return l2_bw_need\n",
    "\n",
    "def gemm_lds_bw(bpe: int = 2, tile_k: int =128,  tile_size : tuple = (128,128), alu_rate: int =2048):\n",
    "    num_flops = tile_size[0]*tile_size[1]*tile_k*2\n",
    "    #4 waves / WG ; 1 WG/CU work dimension \n",
    "    #squared tile requried 2x read-size\n",
    "    bytes_to_read = 2*bpe*(tile_size[0]*tile_k + tile_size[1]*tile_k)\n",
    "    bytes_to_write = bpe*(tile_size[0]*tile_size[1])\n",
    "    lds_bw_need =  (bytes_to_read + bytes_to_write)/(num_flops//alu_rate)\n",
    "    return lds_bw_need\n",
    " \n",
    "def gemm_eff(bpe: int =2, Mtile: int = 128, Ntile: int = 128,  K: int=1024, gpu_name: str = \"mi300\"):\n",
    "    _config = system_config[gpu_name]\n",
    "    l2_bw_required = gemm_l2_bw(K,tile_size=(Mtile,Ntile),bpe=bpe,alu_rate=_config['fp16_rate'])\n",
    "    lds_bw_required = gemm_lds_bw(bpe=bpe,tile_size=(Mtile,Ntile),alu_rate=_config['fp16_rate'])\n",
    "    bw_per_cu = _config['l2_rdbw']/_config['num_cus']\n",
    "    #print(f\"gemm_eff:: lds_bw_required= {lds_bw_required}\")\n",
    "    #print(f\"gemm_eff:: l2_bw_required= {l2_bw_required}\")\n",
    "    gemm_eff = min(bw_per_cu/l2_bw_required, 1)\n",
    "    gemm_eff = min(gemm_eff,_config['lds_bw']/lds_bw_required)\n",
    "    gemm_eff = gemm_eff - (launch_setup_eff_loss/100)\n",
    "    return gemm_eff\n",
    "    \n",
    "def  flashattn_eff(num_wgs_cu:int = 2, bpe:int = 2, attn_dim: int = 128, qtile: int = 128, ktile: int = 128, vtile: int=128, gpu_name:str = \"mi300\"):\n",
    "     _config = system_config[gpu_name]\n",
    "     softmax_cycles = softmax_rate(qtile,ktile)\n",
    "     softmax_per_cycles = qtile*ktile//softmax_cycles\n",
    "     #QKgEMM eff\n",
    "     #qtile is read once in persistent in register or shared mem\n",
    "     #qtile load is amortized with Ktensor*qtile*2*attn_dim math\n",
    "     kbytes_read = ktile*bpe*attn_dim\n",
    "     num_flops = qtile*ktile*attn_dim*2\n",
    "     l2bw_req = kbytes_read/(num_flops//_config['fp16_rate'])\n",
    "     bw_per_cu = math.floor(_config['l2_rdbw']/_config['num_cus'])\n",
    "     #print(f\"bw_per_cu = {bw_per_cu} kbytes_read = {kbytes_read} l2bw_req= {l2bw_req} num_flops = {num_flops}\")\n",
    "     qkgemm_eff = min(bw_per_cu/l2bw_req, 1)\n",
    "     qk_cycles = num_flops//_config['fp16_rate']\n",
    "     qk_elements_per_cycle = qtile*ktile/(qk_cycles/qkgemm_eff)\n",
    "     #print(f\"qk_elements_per_cycle {qk_elements_per_cycle}\")\n",
    "     vbytes_read = vtile*attn_dim*bpe\n",
    "     num_flops= qtile*vtile*attn_dim*2\n",
    "     l2bw_req = vbytes_read/(num_flops//_config['fp16_rate'])\n",
    "     pvgemm_eff = min(bw_per_cu/l2bw_req,1)\n",
    "     pv_cycles = num_flops//(_config['fp16_rate'])\n",
    "     #print(f\"pvgemm_eff = {pvgemm_eff}\")\n",
    "     #print(f\"qkgemm_eff = {qkgemm_eff}\")\n",
    "     #print(f\"softmax_cycles = {softmax_cycles}\")\n",
    "     #print(f\"qk_cycles = {qk_cycles}\")\n",
    "     #print(f\"pv_cycles = {pv_cycles}\")\n",
    "     qksoftmax_eff = min(softmax_per_cycles/qk_elements_per_cycle,1)\n",
    "     #print(f\"qksoftmax_eff = {qksoftmax_eff}\")\n",
    "     if (num_wgs_cu >=2):\n",
    "         fa_eff = qksoftmax_eff * 0.8   #GEMM efficiencies are about .8 for head-dim=128 \n",
    "     elif num_wgs_cu >= 1:\n",
    "         fa_eff = (qk_cycles+pv_cycles)/((qk_cycles/qkgemm_eff)+softmax_cycles*(2-num_wgs_cu)+(pv_cycles/pvgemm_eff))\n",
    "     else:\n",
    "         fa_eff = num_wgs_cu*(qk_cycles+pv_cycles)/((qk_cycles/qkgemm_eff)+softmax_cycles+(pv_cycles/pvgemm_eff)) \n",
    "     print(f\"fa_eff {fa_eff}\")\n",
    "     return fa_eff\n",
    "    \n",
    "    \n",
    "def softmax_rate(qtile: int = 128, ktile: int=128):\n",
    "    #number of ops required for softmax \n",
    "    #max3 -> reduction(wave32 reduction) -> max,  sub(x-max) -> exp() -> rowsum -> fma   \n",
    "    #print(f\" qtile = {qtile} ktile= {ktile}\")\n",
    "    number_softmax_per_wave = qtile*ktile//4/64    #64 threads/wave 4= simds/CU\n",
    "    #print(f\"number_softmax_per_wave = {number_softmax_per_wave}\")\n",
    "    number_max3_wave =  number_softmax_per_wave//2    #max3 = max(x,max(y,z))  \n",
    "    _max3_cycles = number_max3_wave * 4   #16 elements/cycle/simd  \n",
    "    _max3_reduction = 48  # max3 reduction across 32 threds using ds_bpermute (latency)\n",
    "    #sub(x-max) using fma \n",
    "    fma_cycles = number_softmax_per_wave * 2   # 32 ops per cycle/simd  \n",
    "    exp_cycles = number_softmax_per_wave * 16  # exp2f32()  4 ops per cycle/simd\n",
    "    rowsum_cycles = number_softmax_per_wave * 2  # 32 ops per cycle/simd\n",
    "    rowsum_reduction = 48  # ds_bpermute\n",
    "    scaling_cycles = number_softmax_per_wave/2 * 4 # 16 ops per cycle/simd #pk_mul\n",
    "    total_cycles = _max3_cycles + _max3_reduction + fma_cycles + exp_cycles\n",
    "    total_cycles += rowsum_cycles + rowsum_reduction + scaling_cycles\n",
    "    print(f\"softmax_cycles = {total_cycles}\")\n",
    "    return total_cycles\n",
    "\n",
    "def ret_ktile(bpe: int=2, tile_sizes:tuple = (128,128), gpu_name: str = \"mi300\"):\n",
    "    for _val in tilek_sizes:\n",
    "        if ((tile_sizes[0]+tile_sizes[1])*bpe*_val <=  (system_config[gpu_name]['lds_cap']*1024)//2):\n",
    "            return _val\n",
    "        else:\n",
    "            continue\n",
    "    assert(0)\n",
    "\n",
    "def return_nearestpowerof2(N: int) -> int:\n",
    "    if (N  and not (N and N-1)):\n",
    "        return N\n",
    "    else:\n",
    "        a=1\n",
    "        while(pow(2,a) < N):\n",
    "            a +=1\n",
    "        return pow(2,a)\n",
    "\n",
    "\n",
    "def find_bestfaperformance(q_len: int, k_len: int, attn_dim: int, num_heads: int , bpe: int = 2, gpu_name: str=\"mi300\"):\n",
    "    \n",
    "    #2D shard per GPU for num_heads x num_query\n",
    "    num_cus = system_config[gpu_name]['num_cus'] / num_heads\n",
    "    tile_size= q_len / num_cus\n",
    "    tile_size= return_nearestpowerof2(tile_size)\n",
    "    q_tiles = q_len/tile_size\n",
    "    num_fatiles = q_tiles * num_heads\n",
    "    fatiles_percu = np.round(num_fatiles / system_config[gpu_name]['num_cus'],1)\n",
    "    qtile_size = tile_size\n",
    "    #LDS memory to hold K/V elements; Q tile is persistent in register\n",
    "    #64KB LDS size 2 WG(S) holding K elements  \n",
    "    kvtile_size = 64*1024 // (2*attn_dim * bpe)\n",
    "    #print(tile_size,num_fatiles,fatiles_percu)\n",
    "    eff = flashattn_eff(fatiles_percu,bpe,attn_dim,qtile_size,kvtile_size,kvtile_size,gpu_name)\n",
    "    return eff\n",
    "\n",
    "\n",
    "def find_bestgemmperformance(bpe: int =2, M: int = None, N: int =None, K:int =None, gpu_name: str = \"mi300\"):\n",
    "    assert(K!=None)\n",
    "    assert(M!=None)\n",
    "    assert(N!=None)\n",
    "    \n",
    "    if M < 16 or N< 16:\n",
    "       cycles_taken=membound_gemm(M,N,K)\n",
    "       #print(f\"cycles_taken = {cycles_taken}\")\n",
    "       time_taken = cycles_taken/(system_config[gpu_name]['fmax'] * 1e6)\n",
    "       return time_taken\n",
    "    _efficiency_table = []\n",
    "    for tile_size in tile_sizes:\n",
    "        #ktile = ret_ktile(bpe,tile_size,gpu_name)\n",
    "        #print(f\"tile_size[0:1] = {tile_size[0],tile_size[1]}\")\n",
    "        _effmetrics = {}\n",
    "        _effmetrics['tile_size'] = tile_size\n",
    "        _effmetrics['eff'] = gemm_eff(bpe,tile_size[0],tile_size[1],K,gpu_name)\n",
    "        _effmetrics['GPU_util'] = GPU_util(gpu_name,M,N,tile_size[0],tile_size[1])\n",
    "        #print(f\"tile_size[0:1] = {tile_size[0],tile_size[1]}  eff = {_effmetrics['eff']} GPU_util= {_effmetrics['GPU_util']} \")\n",
    "        _efficiency_table.append(_effmetrics)\n",
    "    \n",
    "    assert(len(_efficiency_table)>0)\n",
    "        \n",
    "    pick_winner = []    \n",
    "    for _item in _efficiency_table:\n",
    "        metrics = _item\n",
    "        pick_winner.append(metrics['eff']*metrics['GPU_util'])\n",
    "    \n",
    "    winner_idx = np.argmax(pick_winner)\n",
    "    #print(f\"GEMM_winner:: {_efficiency_table[winner_idx]['eff']}\")\n",
    "    eff = _efficiency_table[winner_idx]['GPU_util'] * _efficiency_table[winner_idx]['eff']\n",
    "    fprate =  system_config[gpu_name]['num_cus'] * system_config[gpu_name]['fp16_rate']\n",
    "    cycles_taken = 2*M*N*K/(eff*fprate)\n",
    "    #print(f\"cycles_taken = {cycles_taken}\")\n",
    "    write_cyles = M*N*bpe/(.8*27*304)\n",
    "    cycles_taken = write_cyles + cycles_taken\n",
    "    time_taken = cycles_taken/(system_config[gpu_name]['fmax'] * 1e6)  + 0.005 \n",
    "    return time_taken\n",
    "    \n",
    "         \n",
    "    #print(f\"GEMM_winner:: {_efficiency_table[winner_idx]['eff']}\")\n",
    "    #return _efficiency_table[winner_idx]\n",
    "\n",
    "def GPU_util(gpu_name: str = 'mi300', gemm_m : int = 1024, gemm_n : int = 1024, mtile:int =128, ntile:int =128):\n",
    "    \n",
    "    num_cus = system_config[gpu_name]['num_cus']\n",
    "    m_tiles = gemm_m/mtile\n",
    "    n_tiles = gemm_n/ntile\n",
    "    tiles_per_cu = m_tiles*n_tiles / num_cus\n",
    "    tiles_per_cu = tiles_per_cu/(math.ceil(tiles_per_cu))\n",
    "    return tiles_per_cu\n",
    "\n",
    "def layerNormCycles(bs:int = 1, seq_len:int = 1, model_dim:int = 1024):\n",
    "    #layernorm ops\n",
    "    # normalize on last two dimension of tensor\n",
    "    #mean = torch.mean(tensor[-1,:,:],dim=-1)\n",
    "    #var  = torch.square(tensor[-1,:,:]-mean).mean(dim=-1)\n",
    "    # layer_norm = tensor[-1,:,:] - mean / (torch.sqrt(var)\n",
    "\n",
    "    #welford online layernorm\n",
    "    #1. pow(x), 2. sum 3. shuffle-op, 4.wglevelreduction, 5. sync() 6. final LN=2 ops.\n",
    "    # use_dowrdx4 4096 bytes/inst 32 cycles/inst (issue_rate)\n",
    "    # grid_size / GPU [batch_size * seq_len,1,1]\n",
    "    #check BW if op-limited (calculate op cycles / payload size)\n",
    "    # if op-cycles < payload_size_latency then return we should able to\n",
    "    # stream benchmark otherwise fraction\n",
    "    # because too many ops involved in LN , memclk might be running lower than optimal frequency (ignore that for now) \n",
    "    mean_cycles = (model_dim/64)*2  # 2 ops for mean\n",
    "    var_cycles =  (model_dim/64)*2  # 2 ops for variance\n",
    "    \n",
    "    wave_level_reduction = np.log2(64) * 4\n",
    "    WG_level_reduction = 48 + 16   # write to LDS and each wave does final mean, variance reduction \n",
    "                                   # 48 cycles pipe latency \n",
    "    final_LN  = (model_dim/64)*3\n",
    "    \n",
    "    total_cycles = final_LN + mean_cycles + var_cycles + wave_level_reduction + WG_level_reduction\n",
    "    #print(f\"total_cycles_per_token_per_cu={total_cycles}\")\n",
    "    bw_gpu = membw_acheived()\n",
    "    #bw_gpu_per_cu_clk = bw_gpu / system_config['mi300']['fmax']\n",
    "    #bytes_per_cu = bw_gpu_per_cu_clk/system_config['mi300']['num_cus']\n",
    "    payload_latency = (model_dim*4*bs*seq_len/(1e6 *bw_gpu)) # layer_norm in float32\n",
    "    \n",
    "    return payload_latency\n",
    "    \n",
    "    #if total_cycles < payload_latency:\n",
    "    #    return bw_gpu_per_cu_clk  \n",
    "    #else:\n",
    "        #mmight not have enough mem instructions in pipe to achieve BW\n",
    "    #    return (payload_latency/total_cycles)*bw_gpu_per_cu_clk  \n",
    "    \n",
    "def geluCycles(model_dim: int = 1024):\n",
    "    #gelu involves with transcendental ops and cubic-root ops.\n",
    "    #gelu =   0.5*tensor * (1.0f + torch.tanh(torch.sqrt(2/numpy.pi)*(x+0.044715*x*x*x)))\n",
    "    # tanh = exp(2.0*x)- 1.0f/ exp(2.0*x) + 1.0f\n",
    "    # tanh(x) = 1 exp() + 1 rcp + 4 fma  + 2 conditional check = 48 cycles\n",
    "    # x = 5 fma \n",
    "   # 3 fma + 1 add\n",
    "   # 2 mov instructions for constants\n",
    "   # total =  12 fma + 1 add + 1 exp + 1 rcp + few mov and conditional check instructions\n",
    "   #     100 =  48 cycles + 4 cycles + 16 cycles + 8 cycles + 8 cycles + 16 cycles \n",
    "   #     \n",
    "   # approx ~100 cycles\n",
    "   # mi300 f32 floats rate = 256/cycle/cu \n",
    "   #                       = 256/100 cycles = 2.5 \n",
    "   # GEMM -rate (floats/cycle/cu) = MN/2*M*N*K/2048 = 2048/2*K = 1024/K  \n",
    "   #                       GEMM_K must be > 1024/2.5  for GELU is not critical path for MLP1\n",
    "\n",
    "   num_float_elements = 256  #thread-items per WG \n",
    "   num_cycles_per_256 = 100   #approximately 100 cycles for 256 elements\n",
    "   return (model_dim/num_float_elements)*num_cycles_per_256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference modeling of LLM for AMD GPU\n",
    "Below is detailed (somewhat)  modeling of LLM for inference tasks for AMD GPUs using 2D parallelism (pipeline and tensor parallelism). for each operator of layers, I am using VALU/Matrix ops requires to implement layer kernels using GPU multi-thread programming. algorithm are little more accurate than simple roofline (fma ops).\n",
    "\n",
    "Analysis is done for mi300 , latest from AMD. Most of the perf numbers are already out in the publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peak memory assertions for various system Components\n",
    "# for running  large models using single GPU + CPU memory + disk CAche\n",
    "\n",
    "def GPU_PeakMem(Phase : MODELPHASE = MODELPHASE.TOKEN.value, ModelConfig: dict = None, probMem: dict = None, batch_size: int =1, sysCfg: dict() = None) -> bool:\n",
    " \n",
    "    \"\"\" Provide GPU peak memory (in elements) required for given model size & model phase\"\"\"\n",
    "    \n",
    "    if probMem is None:\n",
    "        raise ValueError(\"ProbMem is None\")\n",
    "\n",
    "    dim1 = ModelConfig[\"hidden_dim1\"]\n",
    "    dim2 = ModelConfig[\"hidden_dim2\"]\n",
    "    seqlen = ModelConfig[\"max_seq_len\"] \n",
    "    outToken = ModelConfig[\"out_tokens\"]\n",
    "    numLayers = ModelConfig[\"num_layers\"]\n",
    "    numHeads = ModelConfig[\"num_heads\"]\n",
    "    \n",
    "    GpuMem_resident_layer = probMem[\"WeightsGpu\"] * 4*pow(dim1,2) + 2*dim1*dim2  + probMem[\"KVCacheGpu\"]*2*(seqlen + outToken)*dim1\n",
    "                                    \n",
    "    resident_act = ProbMem[\"ActGpu\"]*batch_size*seqlen*dim1 if Phase == MODELPHASE.PREFILL.value else ProbMem[\"ActGpu\"]*batch_size*1*dim1\n",
    "    GpuMem_resident = GpuMem_resident_layer * numLayers + resident_act\n",
    "    \n",
    "    #Working memory for each op\n",
    "    #QKV tensor matrices seqlen x dim1 [3] + input seqlen * dim1  \n",
    "    #for Token generation seqlen = 1\n",
    "    QKV = batch_size*(4*seqlen*dim1) if Phase == MODELPHASE.TOKEN.value else batch_size*(4*dim1)\n",
    "    if Phase == MODELPHASE.PREFILL.value:\n",
    "        #QK inputs + attention outputs\n",
    "        QK_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(seqlen*dim1 + seqlen*seqlen*numHeads + seqlen*dim1)\n",
    "        QKV_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(seqlen*dim1 + seqlen*seqlen*numHeads + seqlen*dim1)\n",
    "    else:\n",
    "        #QK inputs Q seqlen =1 K seqlen (kv cache) \n",
    "        QK_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(dim1 + 1*(seqlen+outToken)*numHeads + (seqlen+outToken)*dim1)\n",
    "        QKV_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(1*dim1 + 1*(seqlen+outToken)*numHeads + (seqlen+outToken)*dim1)   \n",
    "        \n",
    "    Embed = batch_size*(seqlen*dim1*2) if Phase == MODELPHASE.PREFILL.value else  batch_size * (2*dim1)\n",
    "    MLP1 = batch_size*(seqlen*dim1 + seqlen*dim2) if Phase == MODELPHASE.PREFILL.value else batch_size*(1*dim1 + 1*dim2)\n",
    "    MLP2 = batch_size*(seqlen*(dim1+dim2)) if Phase == MODELPHASE.PREFILL.value else batch_size*(1*dim1 + 1*dim2)\n",
    "    \n",
    "    #Total working memory by adding xfered weights stored higher hierarchy \n",
    "    # activation stored in hierachy memory\n",
    "    # max(all the ops in one layer)\n",
    "    workingMem = (1 - probMem[\"WeightsGpu\"])*(4*pow(dim1,1) + 2*dim1*dim2) +\\\n",
    "                (1 - probMem[\"ActGpu\"])*batch_size*seqlen*dim1 + \\\n",
    "                 max(QKV,QK_attention,QKV_attention,Embed,MLP1,MLP2)\n",
    "                 \n",
    "    PeakMemory = workingMem + GpuMem_resident\n",
    "    assert(PeakMemory < sysCfg['gpuMemCapacity'])\n",
    "    return True\n",
    "    \n",
    "\n",
    "def  CPU_PeakMem(Phase : MODELPHASE = MODELPHASE.TOKEN.value, \n",
    "                 ModelConfig: dict = None, probMem: dict = None, \n",
    "                 batch_size: int =1, \n",
    "                 sysCfg: dict = None) -> bool:        \n",
    "    \n",
    "    \"\"\" \n",
    "    Provide CPU peak memory (in elements) required for given model size & model phase\n",
    "    \"\"\"\n",
    "    \n",
    "    if ProbMem is None:\n",
    "        raise ValueError(\"ProbMem is None\")\n",
    "    \n",
    "    dim1 = ModelConfig[\"hidden_dim1\"]\n",
    "    dim2 = ModelConfig[\"hidden_dim2\"]\n",
    "    seqlen = ModelConfig[\"max_seq_len\"] \n",
    "    outToken = ModelConfig[\"out_tokens\"]\n",
    "    numLayers = ModelConfig[\"num_layers\"]\n",
    "    #numHeads = ModelConfig[\"num_heads\"]\n",
    "    \n",
    "    CpuMem_resident_layer = probMem[\"WeightsCpu\"] * (4*pow(dim1,2)) + 2*dim1*dim2  + probMem[\"KVCacheCpu\"]*2*(seqlen + outToken)*dim1\n",
    "    resident_act = ProbMem[\"ActCpu\"]*batch_size*seqlen*dim1 if Phase == MODELPHASE.PREFILL.value else ProbMem[\"ActGpu\"]*batch_size*1*dim1\n",
    "    CpuMem_resident = CpuMem_resident_layer * numLayers + resident_act\n",
    "    \n",
    "    #working memory stored in disk through CPU\n",
    "    #Total working memory by adding xfered weights stored higher hierarchy \n",
    "    # activation stored in hierachy memory\n",
    "    # max(all the ops in one layer)\n",
    "    workingMem = (1 - probMem[\"WeightsCpu\"])*(4*pow(dim1,1) + 2*dim1*dim2) +\\\n",
    "                (1 - probMem[\"ActCpu\"])*batch_size*seqlen*dim1 \n",
    "    \n",
    "    PeakMemory = workingMem + CpuMem_resident\n",
    "    assert(PeakMemory < sysCfg['gpuMemCapacity'])\n",
    "    return True\n",
    "    \n",
    "def  disk_PeakMem(ModelConfig: dict = None, \n",
    "                  probMem: dict = None, \n",
    "                  batch_size: int =1, \n",
    "                  sysCfg: dict = None) -> bool:\n",
    "    \n",
    "    if ProbMem is None:\n",
    "        raise ValueError(\"ProbMem is None\")\n",
    "    dim1 = ModelConfig[\"hidden_dim1\"]\n",
    "    dim2 = ModelConfig[\"hidden_dim2\"]\n",
    "    seqlen = ModelConfig[\"max_seq_len\"] \n",
    "    outToken = ModelConfig[\"out_tokens\"]\n",
    "    numLayers = ModelConfig[\"num_layers\"]\n",
    "    \n",
    "    disk_resident_layer = probMem[\"WeightsDisk\"] * (4*pow(dim1,2)) + 2*dim1*dim2  + probMem[\"KVCacheDisk\"]*2*(seqlen + outToken)*dim1 \n",
    "    resident_act = ProbMem[\"ActDisk\"]*batch_size*seqlen*dim1 \n",
    "    PeakMemory = disk_resident_layer * numLayers + resident_act\n",
    "\n",
    "    assert(PeakMemory < sysCfg['gpuMemCapacity'])\n",
    "    return True\n",
    "    \n",
    "def activationmem_per_layer(batch_size:int, seq_len: int, model_dim1: int, model_dim2: int, FA: bool= True):\n",
    "    #input seqlen*batch_size*model_dim \n",
    "    #Q,K,V tensor(s) = 3*batch_size*seq_len*model_dim\n",
    "    #output projection = seqlen*batch_size*model_dim\n",
    "    #MLP1 = seqlen*model_dim1*\n",
    "    #MLP2 = seqlen*model_dim\n",
    "    #layernorm = seqlen*batch_size*model_dim \n",
    "    # max (above all)\n",
    "    # IF FA == True, no QK output, no dropout,..\n",
    "    input = batch_size*seq_len*model_dim1\n",
    "    QKV_tensor = batch_size*seq_len*model_dim1*3\n",
    "    attn_output = batch_size*seq_len*model_dim1\n",
    "    MLP1_output = batch_size*seq_len*model_dim2\n",
    "    MLP2_output = batch_size*seq_len*model_dim1\n",
    "    layer_norm = batch_size*seq_len*model_dim1\n",
    "    \n",
    "    activation_mem = max(input,QKV_tensor,attn_output,MLP1_output,MLP2_output,layer_norm)\n",
    "    return activation_mem\n",
    "\n",
    "def kvcache_per_layer(Phase: MODELPHASE = MODELPHASE.PREFILL.value,\n",
    "                      batch_size:int =1, \n",
    "                      outTokenSize: int=1, \n",
    "                      seq_len:int = 1, \n",
    "                      hidden_dim:int =1):\n",
    "    \n",
    "    if Phase == MODELPHASE.PREFILL.value:\n",
    "       return 2*(seq_len*batch_size*hidden_dim)\n",
    "    else:   \n",
    "       return (2*(seq_len+outTokenSize)*batch_size*hidden_dim)\n",
    "   \n",
    "        \n",
    "def model_parameters(model_name:str, component:str):\n",
    "    \n",
    "    if model_name in MODEL_CLASSES:\n",
    "        modelDict = MODEL_CLASSES[model_name]\n",
    "    else:\n",
    "        raise ValueError(f\" Model {model_name} not supported yet\")    \n",
    "    if component == MODELCOMPONENTS.Weights.value:\n",
    "        ## 4*hidden_dim**2 + 2*model_dim*hidden_dim + 2*hidden_dim * num_layers + 2*(seqlen + vocab_size)*model_dim  \n",
    "        num_parameters_ = (4*pow(modelDict[\"model_dim\"],2) + 2*modelDict[\"model_dim\"]*modelDict[\"hidden_dim\"] + 2*modelDict[\"model_dim\"]) *modelDict[\"num_layers\"]\n",
    "        num_parameters_ +=  (modelDict[\"max_seq_len\"]+modelDict[\"vocab_size\"])*modelDict[\"model_dim\"]\n",
    "        return num_parameters_\n",
    "    if component == MODELCOMPONENTS.Activation.value:\n",
    "        activation_ = activationmem_per_layer(batch_size=modelDict[\"batch_size\"], seq_len = modelDict[\"max_seq_len\"],model_dim1=modelDict[\"model_dim\"],model_dim2=modelDict[\"hidden_dim\"])\n",
    "        return activation_\n",
    "        \n",
    "    if component == MODELCOMPONENTS.KVcache.value:\n",
    "        kvCache_ = kvcache_per_layer(batch_size=modelDict[\"batch_size\"], seq_len = modelDict[\"max_seq_len\"], hidden_dim=modelDict[\"hidden_dim\"], outTokenSize=modelDict[\"out_tokens\"])\n",
    "        return kvCache_\n",
    "        \n",
    "    raise ValueError(f\" not supported {component} yet\")\n",
    "\n",
    "def qkv_projection_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value):\n",
    "    return batch_size*3*2*modelCfg['max_seq_len']*modelCfg['model_dim']*modelCfg['model_dim']\n",
    "\n",
    "def MLP_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value):\n",
    "    return 2*2*batch_size*modelCfg['max_seq_len']*modelCfg['model_dim']*modelCfg['hidden_dim']\n",
    "\n",
    "def out_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value):\n",
    "    return batch_size*2*modelCfg['max_seq_len']*modelCfg['model_dim']*modelCfg['model_dim']\n",
    "\n",
    "def flashAttention_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value, CMasking:bool = True):\n",
    "    if Phase == MODELPHASE.PREFILL.Value:       \n",
    "        FA = 2*2*batch_size*modelCfg['max_seq_len']*modelCfg['max_seq_len']*modelCfg['model_dim']\n",
    "        FA = FA//2 if CMasking == True else FA\n",
    "        return FA\n",
    "    else:\n",
    "        FA = 2*2*batch_size*modelCfg['max_seq_len']*modelCfg['model_dim']\n",
    "        FA = FA//2 if CMasking == True else FA\n",
    "        return FA\n",
    "\n",
    "def GPUPeakMem(model_name:str):\n",
    "    weights= model_parameters(model_name,\"weights\")\n",
    "    activation = model_parameters(model_name,\"activation\")\n",
    "    kvcache = model_parameters(model_name,\"KVcache\")\n",
    "    \n",
    "    peakMem = weights+activation+kvcache\n",
    "    return peakMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MESH shard (x,y)\n",
    "#pipeline parallelism\n",
    "#split layers among N accelerators\n",
    "\n",
    "#model parallelism (tensor parallelism)\n",
    "#attention - split #heads N accelerators\n",
    "#MLP - split tensors(weight) N accelerators\n",
    "# num_heads = h  hd = d_model/h We\n",
    "# Wq,Wk,Wv  (num_heads,d_modelxhd) \n",
    "# each layer algorithm\n",
    "#  S= Q@K => P = softmax(S) => P = P @ V => O = P @ W_o   \n",
    "# send d_model/N to N-1 accelerators \n",
    "# receive d_model/N from N-1 accelerators\n",
    "# add d_model/N vectors from N-1 accelerators \n",
    "\n",
    "#MLP weights 4*d_model / N split\n",
    "#MLP1 = d_model @ 4*d_model/N => 4*d_model/N @ d_model \n",
    "# send 4*d_model/N to N-1 accelerators\n",
    "# receive 4*d_model/N from accelerators\n",
    "# in total each layer require 4*communication ops\n",
    "\n",
    "#batch=1 case\n",
    "# mem_time (math is boud by memory BW) = bpe*model_parameters/ (N*mi300_bw)\n",
    "# communication = 4*num_layers*8us (latency due to BS=1)  #8 us latency\n",
    "\n",
    "#batch = large\n",
    "# compute = batch_size*2*P/(N*mi300_fp16_flops)\n",
    "# communication =  B * 2 * 4  * (N-1) (model_dim) / (N * link_bw)\n",
    "\n",
    "#flops:mem ratio should give about batch-size \n",
    "# flops : link_bw should give about embedding dimension\n",
    "\n",
    "#parallelism communication happens at four steps\n",
    "# want to parallelize communication and compute\n",
    "#           QKV                      WO                       MLP0               MLP1\n",
    "#  flops    3*B*(d_model*d_model)    B*(d_model_d_model)      4*B*(d_model)       4*B*(d_model*d_model)\n",
    "#  commu    B*(d_model)              b*(d_model)              4*B*d_model         4*B*d_model\n",
    "#  flop/comm  3*d_model              d_model                  4*d_model           4*d_model\n",
    "\n",
    "# embedding dimenssion should be > flops:link BW  ratio of given configuration\n",
    "# chooese batch_size that is optimal for compute-bound to optimze per request latency\n",
    "\n",
    "#pipeline parallelism\n",
    "#batchsize=1 case\n",
    "\n",
    "#worker0   |       |        |         |\n",
    "#          |  B    |        |         |\n",
    "#          |       |        |         |\n",
    "#-------------------------------------\n",
    "#worker1   |       |        |         |\n",
    "#          |       |   B    |         |\n",
    "#          |       |        |         |\n",
    "#--------------------------------------\n",
    "#worker2   |       |        |         |\n",
    "#          |       |        |   B     |\n",
    "#          |       |        |         |\n",
    "\n",
    "#N workers 1 Batch\n",
    "#total slots = N*N\n",
    "#idle slots  = N*(N-1)\n",
    "\n",
    "#idle time = (N-1)/(N)\n",
    "# utilization = 1- idle time\n",
    "\n",
    "#worker0   |     |    |    |    |\n",
    "#          | B0  | B1 |    |    |\n",
    "#          |     |    |    |    |\n",
    "#-------------------------------------\n",
    "#worker1   |     |    |    |    |         \n",
    "#          |     | B0 | B1 |    |         \n",
    "#          |     |    |    |    |         \n",
    "#--------------------------------------\n",
    "#worker2   |     |    |    |    |\n",
    "#          |     |    | B0 | B1 |\n",
    "#          |     |    |    |    |\n",
    "\n",
    "#K minibatches N workers \n",
    "# N+K-1 steps per worker\n",
    "# N*(N+K-1) total steps\n",
    "# N* (N-1) bubbles\n",
    "\n",
    "#idle bubble time = (N-1) / (N+K-1)  \n",
    "\n",
    "#L layers per worker\n",
    "\n",
    "# idle bubble time = (N-1)/(L *(N+K-1))\n",
    "\n",
    "\n",
    "#flops of the model = 24*n_layers * d_model*d_model\n",
    "#attention layer flops\n",
    "#layer norm for every attention layer\n",
    "#de-embedding layer(s)\n",
    "# activations , biases and dropouts\n",
    "\n",
    "#list of operators Encoder\n",
    "# Q,K,V\n",
    "# input Bias\n",
    "# QKT\n",
    "# scaled-softmax\n",
    "# out \n",
    "#output bias\n",
    "# dropout\n",
    "# residual add\n",
    "# layerNorm\n",
    "# MLP0\n",
    "# MLP0 Bias\n",
    "# GELU \n",
    "# MLP1 \n",
    "# MLP1 bias\n",
    "# Residual\n",
    "# layerNorm\n",
    "\n",
    "#add up all memory-bound ops / elements-ops vs GEMM ops\n",
    "# 'other ops' : 'mem-ops'\n",
    "# model size increases by x -times\n",
    "# other_ops * x : x^2 * gemm_ops\n",
    "\n",
    "# % other-ops latency of new model = other_ops(latency) * x / (x^2 * gemm_ops)\n",
    "# if we get same gemm(S) latency as before then = other_ops(latency) / x\n",
    "#actvcation writes out\n",
    "\n",
    "#add communication latency\n",
    "#batch-size = flops/bw \n",
    "\n",
    "#pipeline parallelism\n",
    "#M= micro batches\n",
    "#P= number of devices\n",
    "#tf = forwardtime\n",
    "#tb = backward time\n",
    "# idle time efficiency = p * (p-1) / (m + p)\n",
    "\n",
    "#ideal time = m *(tf+tb)\n",
    "# buble time = (p-1)*(tf+tb) \n",
    "#buble time fraction =(p-1) * (tf+tb) / (m) * (tf+tb)\n",
    "# more stages per GPU (v stages) \n",
    "# tf/v , tb/v\n",
    "# bubble time reduction = p-1 * (tf+tb) / v\n",
    "#tensor model parallelism require all-reduce reduction so use GPU(S) within \n",
    "# GPU server node pipeline parallelism mapped to inter GPU nodes\n",
    "\n",
    "#communication cost per layer for tensor/model parallelism\n",
    "# number of parameters = d\n",
    "# number of devices    = p\n",
    "# network-latency =  alpha\n",
    "# network Bandwidth = B\n",
    "# mini batch-size = b\n",
    "#communcation cost/layer(all-gather) = alpha * log(P) + b*(P=1)*d / (p*B) \n",
    "#             backward =  2* (alpha*log(P) + b*(P-1)*/p*B)\n",
    "#             backward (all-reduct)\n",
    "\n",
    "# batch (data parallelism)\n",
    "# using ring algo for all-reduce (backward) batch -parallel \n",
    "# each device does partial sum followed by all-redce \n",
    "# T (commu-batch)/ layer = 2*(alpha*log[P] + (P-1)/(P*B) * [Weights of the layer])\n",
    "\n",
    "#convolution = Yo * Xi * kh * Kw convolutions production output Yo * Yh *Yw\n",
    "# W = kh*kw*Xc*Yc\n",
    "# di = Yc*Xh*Xw\n",
    "\n",
    "# ratio of communcation volume = batch/model = 2*[Wi]/3*B*(di) \n",
    "\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# |0 | 1 | 2 | 3 |\n",
    "# |4 | 5 | 6 | 7 | \n",
    "# ----------------\n",
    "\n",
    "#pipeline parallelism = X dimension = 4 GPU(S)  layers_per_rank = layers/num_ranks  \n",
    "#tensor parallelism    = Y dimension = 2 GPU(s)  tensors_per_rank  = hidden_dim/num_ranks model_dim/num_ranks  heads_per_rank = attn_heads/num_ranks\n",
    "# data_parallelism    = X dimension = 4 GPU(s)  batchsize_per_rank = batch_size/num_ranks\n",
    "\n",
    "\n",
    "#B = batchsize\n",
    "#N = number of heads\n",
    "#M = model-dimension\n",
    "#S = sequence_len\n",
    "#H = hidden_dimension\n",
    "embedding_shard_BSM = {'batch_size' : 'X', 'seq_len' : '_', 'model_dim': \"_'\"}\n",
    "activation_shard_BSM = {'batch_size'  : 'X', 'seq_len' : '_', 'model_dim' : 'Y'}\n",
    "activation_shard_BSND = {'batch_size' : 'X', 'seq_len': '_', 'attn_heads': 'Y', 'attn_dim' : '_'}\n",
    "activation_shard_BSH = {'batch_size'  : 'X', 'seq_len' : '_', 'hidden_dim' : 'Y'}\n",
    "activation_shard_BNSS = {'batch_size' : 'X', 'attn_heads' : 'Y', 'max_seq_len' : '_'}\n",
    "\n",
    "from dataclasses import dataclass\n",
    "#first key is summation dimension second key is free0/free1 dimension\n",
    "weights_shard_MM  = {'model_dim'  : '_', 'model_dim1' : '_'}\n",
    "weights_shard_MND = {'model_dim'  : '_', 'attn_heads'  : 'Y', 'attn_dim' : '_'} \n",
    "weights_shard_NDM = {'attn_heads' : 'Y', 'attn_dim'  : '_', 'model_dim'  : 'X'}\n",
    "weights_shard_MH  = {'model_dim'  : 'X', 'hidden_dim' : 'Y'}\n",
    "weights_shard_HM  = {'model_dim'  : 'X', 'hidden_dim' : 'Y'}\n",
    "embedding_weight =  {'vocab_size'  : '_', 'model_dim' : '_'}\n",
    "\n",
    "@dataclass\n",
    "class transformer_2dshard:\n",
    "    embedding_activations = {'input0' : embedding_shard_BSM, 'output' : embedding_shard_BSM}\n",
    "    qkv_activations = {'input0' : activation_shard_BSM , 'input1' : weights_shard_MM, 'output' : activation_shard_BSND}    # weights M=8192  N=64 D = 128   activation : B=1x2048x8192  (1x2048x128, 64) \n",
    "    flash_attention_qk = {'input0' : activation_shard_BSND , 'input1' : activation_shard_BSND, 'output' : activation_shard_BNSS}\n",
    "    flash_attention_sv = {'input0' : activation_shard_BSND , 'input1' : activation_shard_BNSS, 'output' : activation_shard_BSND}\n",
    "    out_projection =  {'input0' : activation_shard_BSND , 'input1' :  weights_shard_NDM , 'output' : activation_shard_BSM}\n",
    "    #FIXME sequence_len sharding \n",
    "    layer_norm = {'input' : activation_shard_BSM , 'output' : activation_shard_BSM}\n",
    "    mlp_0 = {'input0': activation_shard_BSM, 'input1' : weights_shard_MH, 'output' : activation_shard_BSH}\n",
    "    gelu  = {'input': activation_shard_BSH, 'output' : activation_shard_BSH}\n",
    "    mlp_1 = {'input1': activation_shard_BSH, 'input1' : weights_shard_HM, 'output' : activation_shard_BSM}\n",
    "    \n",
    "    def __init__(self,model_name:str = \"llama_65B\",\n",
    "                 bpe : int = 2, \n",
    "                 num_gpus : int =8,  \n",
    "                 sysCfg: system_config = None,\n",
    "                 tileSizes: list[tuple] = None):\n",
    "        \n",
    "        self.bpe = bpe\n",
    "        if model_name in MODEL_CLASSES.keys():\n",
    "            self.modelParameters = MODEL_CLASSES[model_name]  \n",
    "            self.model_name = model_name\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model\")\n",
    "        self.numGPUs = num_gpus\n",
    "        \n",
    "        #3D sharding for n GPU(s)\n",
    "        self.shard_topology = (\"pipeline\",\"tensor\")  \n",
    "        self.devices = np.arange(num_gpus)\n",
    "        self.mesh = np.array(self.devices).reshape(-1,2)\n",
    "        \n",
    "        self.systemCfg = sysCfg\n",
    "        self.num_cus = self.systemCfg['mi300']['num_cus']\n",
    "        self.fp16rate = self.num_cus*self.systemCfg['mi300']['fp16_rate']\n",
    "        self.num_layers = self.modelParameters['num_layers']\n",
    "\n",
    "        \n",
    "        self.tileSizeChoices = tileSizes\n",
    "        self.max_seq_len = self.modelParameters['max_seq_len']\n",
    "        \n",
    "        #support 2D mesh for sharding \n",
    "        self.pipeline_num_ranks = self.mesh.shape[0]\n",
    "        self.layers_per_rank  = self.modelParameters['num_layers'] // self.pipeline_num_ranks\n",
    "        self.num_microbatches = self.modelParameters['batch_size']//self.modelParameters['microbatch_size']\n",
    "            \n",
    "        self.tensor_num_ranks = self.mesh.shape[1]\n",
    "        \n",
    "        \n",
    "\n",
    "    def peakmem_check(self):\n",
    "        memUsage = GPUPeakMem(self.model_name)//1e9\n",
    "        #pipeline parallelism takes batch_size dimension\n",
    "        memCapacity = self.systemCfg['mi300']['hbm_cap']\n",
    "        memUsage = memUsage//len(self.mesh[1])\n",
    "        assert(memUsage <= memCapacity)\n",
    "        \n",
    "    def recv_payloadsize(self):\n",
    "        return self.modelParameters['batch_size'] * self.modelParameters['model_dim'] * self.modelParameters['max_seq_len']\n",
    "    \n",
    "    def layer_ops(self,seq_len: int):\n",
    "        \n",
    "        #qkv_projections\n",
    "        act_tensor = {}\n",
    "        wt_tensor = {}\n",
    "        \n",
    "        for key, _ in self.qkv_activations['input0'].items():\n",
    "            if key == 'batch_size':\n",
    "                act_tensor['batch_size'] = self.modelParameters['microbatch_size']\n",
    "            elif key == 'seq_len':\n",
    "                act_tensor['M'] = seq_len\n",
    "            elif key == 'model_dim':\n",
    "                act_tensor['K'] = self.modelParameters['model_dim'] \n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSM tensor\")\n",
    "                \n",
    "        for key, value in self.qkv_activations['input1'].items():\n",
    "            if key == 'model_dim':\n",
    "                wt_tensor['K'] = self.modelParameters['model_dim']\n",
    "            elif key == 'model_dim1':\n",
    "                wt_tensor['N'] = self.modelParameters['model_dim']\n",
    "                if value != '_':\n",
    "                    wt_tensor['N'] = wt_tensor['N']//self.tensor_num_ranks\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSM tensor\")        \n",
    "        #print(f\"fp_rate {self.fp16rate}\")\n",
    "        print(f\"QKV_Projection:: GEMM problem sizes M={act_tensor['M']} N= {3*wt_tensor['N']} K= {act_tensor['K']}\")\n",
    "        #returns efficiency\n",
    "        #efficiency = find_bestgemmperformance(self.bpe,act_tensor['M'],3*wt_tensor['N'],act_tensor['K'],\"mi300\")\n",
    "        qkv_duration = np.round(find_bestgemmperformance(self.bpe,act_tensor['M'],3*wt_tensor['N'],act_tensor['K'],\"mi300\") / 2,4)\n",
    "        #print(f\"qkvgemm_eff : {efficiency['eff']} utilization={efficiency['GPU_util']}\")\n",
    "        #qkv_cycles = ((2*act_tensor['M'])*3*wt_tensor['N']*act_tensor['K'])/(efficiency['GPU_util']*efficiency['eff']*self.fp16rate)\n",
    "        #qkv_cycles = qkv_cycles/self.tensor_num_ranks/1500/1000 \n",
    "        print(f\"QKV_Projection:: time_duration(ms): [{qkv_duration}]\")\n",
    "        #flash attention \n",
    "        #calculate num_heads per GPU\n",
    "        #case 1:  split heads in 'Y' dimension (tensor parallelism)\n",
    "        q_tensor = {}\n",
    "        k_tensor = {}\n",
    "        v_tensor = {}\n",
    "\n",
    "        for key, value in self.flash_attention_qk['input0'].items():\n",
    "            if key == 'batch_size':\n",
    "                q_tensor['batch_size'] = self.modelParameters['microbatch_size']\n",
    "            elif key == 'seq_len':\n",
    "                q_tensor['M'] = seq_len\n",
    "            elif key == 'attn_dim':\n",
    "                q_tensor['K'] = self.modelParameters['model_dim'] // self.modelParameters['attn_heads']\n",
    "            elif key == 'attn_heads':\n",
    "                heads_num_ranks = self.tensor_num_ranks\n",
    "                q_tensor['h']  = self.modelParameters['attn_heads']\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSND tensor\")\n",
    "        \n",
    "        for key, _ in self.flash_attention_qk['input1'].items():\n",
    "            if key == 'batch_size':\n",
    "                k_tensor['batch_size'] = self.modelParameters['microbatch_size']\n",
    "                v_tensor['batch_size'] = self.modelParameters['microbatch_size']\n",
    "            elif key == 'seq_len':\n",
    "                k_tensor['N'] = seq_len\n",
    "                v_tensor['N'] = seq_len\n",
    "            elif key == 'attn_dim':\n",
    "                k_tensor['K'] = self.modelParameters['model_dim'] // self.modelParameters['attn_heads']\n",
    "                v_tensor['K'] = self.modelParameters['model_dim'] // self.modelParameters['attn_heads']\n",
    "            elif key == 'attn_heads':\n",
    "                k_tensor['h']  = self.modelParameters['attn_heads']\n",
    "                v_tensor['h']  = self.modelParameters['attn_heads']\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSND tensor\")\n",
    "                \n",
    "\n",
    "        #shard heads in 'Y' dimension     \n",
    "        q_tensor['h'] = q_tensor['h']//heads_num_ranks\n",
    "        print(f\"FA problem sizes : q_len = {q_tensor['M']} k_len = {k_tensor['N']}, attn_dim={q_tensor['K']}, num_heads={q_tensor['h']}\")\n",
    "        fa_eff = find_bestfaperformance(q_tensor['M'],k_tensor['N'],q_tensor['K'],q_tensor['h'],self.bpe,\"mi300\")\n",
    "        fa_flops = 2*(q_tensor['batch_size']*q_tensor['h']*(q_tensor['M']*q_tensor['K']*k_tensor['N'] + v_tensor['K']*q_tensor['M']*v_tensor['N']))\n",
    "        fa_cycles = np.round(fa_flops/(self.fp16rate*fa_eff),4)\n",
    "        fa_duration = np.round(fa_cycles/(self.systemCfg['mi300']['fmax']*1e6),4)\n",
    "        print(f\"FlashAttention::  seqlen={seq_len} attn_heads= {self.modelParameters['attn_heads']} attn_dim= {q_tensor['K']}\")\n",
    "        print(f\"flash attention:: duration(ms)=[{fa_duration}]\") \n",
    "        #attention result projection output to [B,S,M]\n",
    "        #input [B,S,N,D] x [N,D,M] -> [B,S,M]\n",
    "        #shard on model-dimension\n",
    "        #gather for reduction\n",
    "        o_tensor = {}\n",
    "        wo_tensor = {}\n",
    "        \n",
    "        for key, value in self.out_projection['input0'].items():\n",
    "            if key == 'batch_size':\n",
    "                o_tensor['batch_size'] = self.modelParameters['microbatch_size']\n",
    "            elif key == 'seq_len':\n",
    "                o_tensor['M'] = seq_len\n",
    "            elif key == 'attn_dim':\n",
    "                o_tensor['K'] = self.modelParameters['model_dim']\n",
    "            elif key == 'attn_heads':\n",
    "                heads_num_ranks = self.tensor_num_ranks\n",
    "            else:\n",
    "                print(key)\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSND tensor\")\n",
    "            \n",
    "        for key, value in self.out_projection['input1'].items():\n",
    "            if key == 'model_dim':\n",
    "                wo_tensor['N'] = self.modelParameters['model_dim'] // self.tensor_num_ranks\n",
    "            elif key == 'attn_dim':\n",
    "                wo_tensor['K'] = self.modelParameters['model_dim']\n",
    "            elif key == 'attn_heads':\n",
    "                heads_num_ranks = self.tensor_num_ranks\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in weights_shard_NDM tensor\")    \n",
    "        \n",
    "        #reshape BSND into BSM (gather primitive)    \n",
    "        print(f\"Out_Projection:: GEMM problem sizes M={o_tensor['M']} N= {wo_tensor['N']} K= {o_tensor['K']}\")\n",
    "        #efficiency = find_bestgemmperformance(self.bpe,o_tensor['M'],wo_tensor['N'],o_tensor['K'],\"mi300\")\n",
    "        #print(efficiency['eff'])\n",
    "        #print(efficiency['GPU_util'])\n",
    "        #projout_cycles = (2*o_tensor['batch_size']*o_tensor['M']*wo_tensor['N']*o_tensor['K'])/(efficiency['GPU_util']*efficiency['eff']*self.fp16rate) \n",
    "        #projout_cycles = projout_cycles/1500/1000\n",
    "        #print(f\"out_projection cycles = {projout_cycles}\")\n",
    "        \n",
    "        out_duration = np.round(find_bestgemmperformance(self.bpe,o_tensor['M'],wo_tensor['N'],o_tensor['K'],\"mi300\"),4)\n",
    "        print(f\"out_projection duration(ms) = {out_duration}\")\n",
    "        \n",
    "        payload_size = (self.modelParameters['microbatch_size'] * seq_len)\n",
    "        ln_duration = np.round(2*layerNormCycles(self.modelParameters['microbatch_size'],seq_len,self.modelParameters['model_dim']),4)\n",
    "        print(f\"ln- duration(ms) = {ln_duration}\")\n",
    "        bias_duration = payload_size/(self.num_cus*128)/(self.systemCfg['mi300']['fmax']*1e6)   #rough estimation\n",
    "        #print(f\"bias_duration = {bias_duration}\")\n",
    "        act_tensor = {}\n",
    "        wt_tensor = {}\n",
    "        for key, value in self.mlp_0['input0'].items():\n",
    "            if key == 'batch_size':\n",
    "                act_tensor['batch_size'] = self.modelParameters['microbatch_size']\n",
    "            elif key == 'seq_len':\n",
    "                act_tensor['M'] = seq_len\n",
    "            elif key == 'model_dim':\n",
    "                act_tensor['K'] = self.modelParameters['model_dim']\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSM tensor\")\n",
    "            \n",
    "        for key, value in self.mlp_0['input1'].items():\n",
    "            if key == 'model_dim':\n",
    "                wt_tensor['N'] = self.modelParameters['hidden_dim']\n",
    "                self.tensor_num_ranks  = 1  if value == '_' else self.tensor_num_ranks\n",
    "            elif key == 'hidden_dim':\n",
    "                wt_tensor['K'] = self.modelParameters['model_dim'] \n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in weights_shard_MH tensor\")    \n",
    "         \n",
    "         \n",
    "        print(f\"MLP:: GEMM problem sizes M={act_tensor['M']} N= {wt_tensor['N']} K= {wt_tensor['K']}\")    \n",
    "        wt_tensor['N'] = wt_tensor['N'] // self.mesh.shape[1]\n",
    "        #efficiency = find_bestgemmperformance(self.bpe,act_tensor['M'],wt_tensor['N'],wt_tensor['K'],\"mi300\")\n",
    "        #mlp_cycles = 2*(2*act_tensor['M']*wt_tensor['N']*act_tensor['K'])//(efficiency['GPU_util']*efficiency['eff']*self.fp16rate)\n",
    "        mlp_duration = np.round(find_bestgemmperformance(self.bpe,act_tensor['M'],wt_tensor['N'],wt_tensor['K'],\"mi300\"),4)\n",
    "        #mlp_cycles = mlp_cycles/1500/1000\n",
    "        print(f\"MLP[0-1] duration(ms) :: {mlp_duration}\")\n",
    "        #add reduction \n",
    "        comm_bw_acheived = self.systemCfg['mi300']['comm_bw']*self.systemCfg['mi300']['comm_eff']\n",
    "        comm_bw_per_cycle = comm_bw_acheived / self.systemCfg['mi300']['fmax']                                                                     \n",
    "        \n",
    "        #print(f\"comm_bw_per_cycle = {comm_bw_per_cycle}\")\n",
    "        \n",
    "        #gather cycles\n",
    "        #communication _time\n",
    "        latency_time = np.round(self.systemCfg['mi300']['comm_latency'] * math.log2(self.tensor_num_ranks) / 1000,4)\n",
    "        #gather cycles\n",
    "        gather_duration = np.round(((self.tensor_num_ranks-1)/(self.tensor_num_ranks))*self.layers_per_rank*self.modelParameters['model_dim']*self.bpe / (1e6*comm_bw_acheived),4) + latency_time\n",
    "        #reduction cycles\n",
    "        #scatter cycles (multiplied by layers per device)\n",
    "        reduce_scatter_duration = np.round(act_tensor['M']*self.modelParameters['hidden_dim']*self.bpe / (1e6*comm_bw_acheived),4)\n",
    "        print(f\"reduce_scatter ={reduce_scatter_duration}\")\n",
    "        #bw_per_cycle = (membw_acheived()/self.systemCfg['mi300']['fmax'])\n",
    "        #reduction by reading \n",
    "        reduce_scatter_reduction = 2*reduce_scatter_duration + (3*self.tensor_num_ranks*act_tensor['M']*self.modelParameters['hidden_dim']*self.bpe / (1e9*membw_acheived()) *1e3)  +  latency_time\n",
    "        reduce_scatter_reduction = np.round(reduce_scatter_reduction,4)\n",
    "        gelu_cycles = geluCycles(act_tensor['M']*wt_tensor['N']/self.num_cus)\n",
    "        gelu_duration = np.round(gelu_cycles/(self.systemCfg['mi300']['fmax']*1e6),4)\n",
    "        print(f\"gelu duration(ms) = {gelu_duration}\")\n",
    "        print(f\"reduce_scatter_reduction (ms)= {reduce_scatter_reduction}\")\n",
    "        total_time = qkv_duration + fa_duration  + 2*ln_duration + out_duration\n",
    "        total_time += mlp_duration + gelu_duration + reduce_scatter_reduction + gather_duration\n",
    "            \n",
    "        print(f\" total_time per layer = {total_time}\")\n",
    "        \n",
    "        return total_time\n",
    "        \n",
    "    def forward_phase(self):\n",
    "        #prefill phase\n",
    "        print(\"\\n\")\n",
    "        print(\"######################################\")\n",
    "        print(\"****** Prefill Phase *****************\")\n",
    "        print(\"######################################\")\n",
    "        prefill_time = self.layer_ops(self.max_seq_len) * (self.num_layers/self.pipeline_num_ranks)\n",
    "        print(f\"prefill (ms) = {prefill_time}\")\n",
    "        print(\"\\n\")\n",
    "        print(\"######################################\")\n",
    "        print(\"****** decode Phase *****************\")\n",
    "        print(\"######################################\")\n",
    "        token_gen_time = self.layer_ops(16) * (self.num_layers/self.pipeline_num_ranks)\n",
    "        print(f\"token_gen= {token_gen_time}\")\n",
    "        total_time = np.round((prefill_time + token_gen_time),4)\n",
    "        print(\"######################################\")\n",
    "        print(\"****** Total Time   *****************\")\n",
    "        print(\"######################################\")\n",
    "        print(f\"total_time (ms) = {total_time}\")\n",
    "        #tokens_per_msecond = total_cycles/(self.systemCfg['mi300']['fmax'] * 1000)/1000\n",
    "        #print(tokens_per_msecond)\n",
    "        \n",
    "    def pipeline_eff(self):\n",
    "        #pipeline efficiency\n",
    "        pipeline_idle_fraction = (self.pipeline_num_ranks-1)/(self.pipeline_num_ranks+self.num_microbatches-1)\n",
    "        #layers per device\n",
    "        pipeline_idle = pipeline_idle_fraction/self.layers_per_rank\n",
    "        return pipeline_idle\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "######################################\n",
      "****** Prefill Phase *****************\n",
      "######################################\n",
      "QKV_Projection:: GEMM problem sizes M=2048 N= 24576 K= 8192\n",
      "QKV_Projection:: time_duration(ms): [0.4905]\n",
      "FA problem sizes : q_len = 2048 k_len = 2048, attn_dim=128, num_heads=32\n",
      "softmax_cycles = 3168.0\n",
      "fa_eff 0.5769014084507043\n",
      "FlashAttention::  seqlen=2048 attn_heads= 64 attn_dim= 128\n",
      "flash attention:: duration(ms)=[0.1276]\n",
      "Out_Projection:: GEMM problem sizes M=2048 N= 4096 K= 8192\n",
      "out_projection duration(ms) = 0.1907\n",
      "ln- duration(ms) = 0.0315\n",
      "MLP:: GEMM problem sizes M=2048 N= 22016 K= 8192\n",
      "MLP[0-1] duration(ms) :: 0.4695\n",
      "reduce_scatter =0.2876\n",
      "gelu duration(ms) = 0.0193\n",
      "reduce_scatter_reduction (ms)= 0.7142\n",
      " total_time per layer = 2.0873\n",
      "prefill (ms) = 41.745999999999995\n",
      "\n",
      "\n",
      "######################################\n",
      "****** decode Phase *****************\n",
      "######################################\n",
      "QKV_Projection:: GEMM problem sizes M=16 N= 24576 K= 8192\n",
      "QKV_Projection:: time_duration(ms): [0.0132]\n",
      "FA problem sizes : q_len = 16 k_len = 16, attn_dim=128, num_heads=32\n",
      "softmax_cycles = 118.0\n",
      "fa_eff 0.03779913636996796\n",
      "FlashAttention::  seqlen=16 attn_heads= 64 attn_dim= 128\n",
      "flash attention:: duration(ms)=[0.0001]\n",
      "Out_Projection:: GEMM problem sizes M=16 N= 4096 K= 8192\n",
      "out_projection duration(ms) = 0.0135\n",
      "ln- duration(ms) = 0.0002\n",
      "MLP:: GEMM problem sizes M=16 N= 22016 K= 8192\n",
      "MLP[0-1] duration(ms) :: 0.0198\n",
      "reduce_scatter =0.0022\n",
      "gelu duration(ms) = 0.0002\n",
      "reduce_scatter_reduction (ms)= 0.0174\n",
      " total_time per layer = 0.0771\n",
      "token_gen= 1.542\n",
      "######################################\n",
      "****** Total Time   *****************\n",
      "######################################\n",
      "total_time (ms) = 43.288\n"
     ]
    }
   ],
   "source": [
    "llm2 = transformer_2dshard(sysCfg=system_config,tileSizes=tile_sizes)\n",
    "time = llm2.forward_phase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def welford_update(count, mean, M2, currValue):\n",
    "    count += 1\n",
    "    delta = currValue - mean\n",
    "    mean += delta / count\n",
    "    delta2 = currValue - mean\n",
    "    M2 += delta * delta2\n",
    "    return (count, mean, M2)\n",
    "\n",
    "\n",
    "def naive_update(sum, sum_square, currValue):\n",
    "    sum = sum + currValue\n",
    "    sum_square = sum_square + currValue * currValue\n",
    "    return (sum, sum_square)\n",
    "\n",
    "\n",
    "x_arr = np.random.randn(100000).astype(np.float32)\n",
    "\n",
    "welford_mean = 0\n",
    "welford_m2 = 0\n",
    "welford_count = 0\n",
    "for i in range(len(x_arr)):\n",
    "    new_val = x_arr[i]\n",
    "    welford_count, welford_mean, welford_m2 = welford_update(welford_count, welford_mean, welford_m2, new_val)\n",
    "print(\"Welford mean: \", welford_mean)\n",
    "print(\"Welford var: \", welford_m2 / welford_count)\n",
    "\n",
    "naive_sum = 0\n",
    "naive_sum_square = 0\n",
    "for i in range(len(x_arr)):\n",
    "    new_val = x_arr[i]\n",
    "    naive_sum, naive_sum_square = naive_update(naive_sum, naive_sum_square, new_val)\n",
    "naive_mean = naive_sum / len(x_arr)\n",
    "naive_var = naive_sum_square/ len(x_arr) - naive_mean*naive_mean\n",
    "print(\"Naive mean: \", naive_mean)\n",
    "print(\"Naive var: \", naive_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
