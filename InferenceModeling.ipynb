{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np      \n",
    "import math\n",
    "import argparse\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    \"Gpt3_175B\"  : {\"microbatch_size\": 1, \"batch_size\": 1, \"max_seq_len\": 2048, \"vocab_size\": 64000, \"model_dim\": 12288, \"hidden_dim\": 49152, \"num_layers\" : 96, \"attn_heads\" : 96, \"out_tokens\": 32},\n",
    "    \"llama_65B\"  : {\"microbatch_size\": 1, \"batch_size\": 1, \"max_seq_len\": 2048,\"vocab_size\": 32000, \"model_dim\": 8192, \"hidden_dim\": 22016, \"num_layers\" : 80, \"attn_heads\" : 64, \"out_tokens\": 32},\n",
    "}\n",
    "system_config = {\n",
    "    \"mi300\" : { 'hbm_cap': 192, 'lds_cap': 64,\n",
    "               'num_cus': 304,\n",
    "               'f16_rate' : 2048, 'fmax' : 1.5 , 'l2_rdbw' : 16384,\n",
    "               'l2_wrbw': 8192, 'hbm_bw' : 4403, 'lds_bw' : 128, 'fma_rate' : 256}\n",
    "}\n",
    "launch_setup_eff_loss = 5 \n",
    "NUM_GPUS = 8\n",
    "Bpe = 2   #fp16\n",
    "stream_bw_attainment = .8  #efficiency\n",
    "\n",
    "class MODELCOMPONENTS(Enum):\n",
    "    Weights = \"weights\"\n",
    "    Activation = \"activation\"\n",
    "    KVcache  = \"KVcache\"\n",
    "    MOES = \"Moes\"\n",
    "    \n",
    "class MODELPHASE(Enum):\n",
    "    PREFILL = 0\n",
    "    TOKEN = 1    \n",
    "    \n",
    "class MODELPIPELINE(Enum):\n",
    "    INFERENCE = 0\n",
    "    TRAINING = 1\n",
    "\n",
    "ProbMem = dict()\n",
    "ProbMem['KVCacheGpu'] = 0.5\n",
    "ProbMem['WeightsGpu'] = 0.5\n",
    "ProbMem['ActGpu'] = 0.5\n",
    "ProbMem['KVCacheCpu'] = 0.2\n",
    "ProbMem['WeightsCpu'] = 0.2\n",
    "ProbMem['ActCpu'] = 0.2\n",
    "ProbMem['KVCacheDisk'] = 0.3\n",
    "ProbMem['WeightsDisk'] = 0.3\n",
    "ProbMem['ActDisk'] = 0.3\n",
    "\n",
    "#h100\n",
    "#NVLink = 990 GB/s    #infiniband = 448GB /s  (%75 achievable target)\n",
    "#capacity = 80GB\n",
    "#bf16 = 1979Tflops / 2 (dense)\n",
    "#mem-BW = 3.3TB/s\n",
    "\n",
    "tile_sizes = [(64,64),(128,128),(256,256),(256,128)]\n",
    "tilek_sizes = [1024,512,256,128,64,32]\n",
    "\n",
    "\n",
    "\n",
    "def gemm_l2_bw(gemm_k: int, tile_size : tuple = (128,128), bpe:int =2, alu_rate: int = 2048):\n",
    "    num_flops = tile_size[0]*tile_size[1]*gemm_k*2\n",
    "    bytes_to_read = bpe*(tile_size[0]*gemm_k + tile_size[1]*gemm_k)\n",
    "    bytes_to_write = bpe*(tile_size[0]*tile_size[1])\n",
    "    l2_bw_need =  (bytes_to_read + bytes_to_write)//(num_flops//alu_rate)\n",
    "    return l2_bw_need\n",
    "\n",
    "def gemm_lds_bw(bpe: int = 2, tile_k: int =128,  tile_size : tuple = (128,128), alu_rate: int =2048):\n",
    "    num_flops = tile_size[0]*tile_size[1]*tile_k*2\n",
    "    #4 waves / WG ; 1 WG/CU work dimension \n",
    "    #squared tile requried 2x read-size\n",
    "    bytes_to_read = 2*bpe*(tile_size[0]*tile_k + tile_size[1]*tile_k)\n",
    "    bytes_to_write = bpe*(tile_size[0]*tile_size[1])\n",
    "    lds_bw_need =  (bytes_to_read + bytes_to_write)//(num_flops//alu_rate)\n",
    "    return lds_bw_need\n",
    " \n",
    "def gemm_eff(bpe: int =2, Mtile: int = 128, Ntile: int = 128, Ktile:int = 128, K: int=1024, gpu_name: str = \"mi300\"):\n",
    "    \n",
    "    _config = system_config[gpu_name]\n",
    "    l2_bw_required = gemm_l2_bw(K,tile_size=tuple(Mtile,Ntile),bpe=bpe,alu_rate=_config['fp16_rate'])\n",
    "    lds_bw_required = gemm_lds_bw(bpe=bpe,tile_size=tuple(Mtile,Ntile),alu_rate=_config['fp16_rate'])\n",
    "    bw_per_cu = _config['l2_rdbw']//_config['num_cus']\n",
    "    gemm_eff = min(bw_per_cu//l2_bw_required, 1)\n",
    "    gemm_eff = min(gemm_eff,_config['lds_bw']//lds_bw_required)\n",
    "    gemm_eff = gemm_eff - (launch_setup_eff_loss/100)\n",
    "    return gemm_eff\n",
    "    \n",
    "def  flashattn_eff(num_wgs_cu:int = 2, bpe:int = 2, attn_dim: int = 128, qtile: int = 128, ktile: int = 128, vtile: int=128, gpu_name:str = \"mi300\"):\n",
    "     _config = system_config[gpu_name]\n",
    "     softmax_cycles = softmax_rate(qtile,ktile)\n",
    "     softmax_per_cycles = qtile*ktile//softmax_cycles\n",
    "     #QKgEMM eff\n",
    "     #qtile is read once in persistent in register or shared mem\n",
    "     #qtile load is amortized with Ktensor*qtile*2*attn_dim math\n",
    "     kbytes_read = ktile*bpe*attn_dim\n",
    "     num_flops = qtile*ktile*attn_dim*2\n",
    "     l2bw_req = kbytes_read/(num_flops//_config['fp16_rate'])\n",
    "     bw_per_cu = _config['l2_rdbw']//_config['num_cus']\n",
    "     qkgemm_eff = min(bw_per_cu//l2bw_req, 1)\n",
    "     qk_cycles = num_flops//_config['fp16_rate']\n",
    "     qk_elements_per_cycle = qtile*ktile//(qk_cycles//qkgemm_eff)\n",
    "     vbytes_read = vtile*attn_dim*bpe\n",
    "     num_flops= qtile*vtile*attn_dim*2\n",
    "     l2bw_req = vbytes_read//(num_flops//_config['fp16_rate'])\n",
    "     pvgemm_eff = min(bw_per_cu/l2bw_req,1)\n",
    "     pv_cycles = num_flops//(_config['fp16_rate'])\n",
    "     qksoftmax_eff = softmax_per_cycles//qk_elements_per_cycle\n",
    "     fa_eff = qksoftmax_eff*pvgemm_eff if num_wgs_cu >=2 else (qk_cycles+pv_cycles)//((qk_cycles//qkgemm_eff)+softmax_cycles+(pv_cycles//pvgemm_eff))\n",
    "     return fa_eff\n",
    "    \n",
    "    \n",
    "def softmax_rate(qtile: int = 128, ktile: int=128):\n",
    "    #number of ops required for softmax \n",
    "    #max3 -> reduction(wave32 reduction) -> max,  sub(x-max) -> exp() -> rowsum -> fma   \n",
    "    \n",
    "    number_softmax_per_wave = qtile*ktile//4\n",
    "    number_max3_wave =  number_softmax_per_wave//2    #max3 = max(x,max(y,z))  \n",
    "    _max3_cycles = number_max3_wave * 4   #16 elements/cycle/simd  \n",
    "    _max3_reduction = 48  # max3 reduction across 32 threds using ds_bpermute (latency)\n",
    "    #sub(x-max) using fma \n",
    "    fma_cycles = number_softmax_per_wave * 2   # 32 ops per cycle/simd  \n",
    "    exp_cycles = number_softmax_per_wave * 16  # exp2f32()  4 ops per cycle/simd\n",
    "    rowsum_cycles = number_softmax_per_wave * 2  # 32 ops per cycle/simd\n",
    "    rowsum_reduction = 48  # ds_bpermute\n",
    "    scaling_cycles = number_softmax_per_wave * 4 # 16 ops per cycle/simd\n",
    "    total_cycles = _max3_cycles + _max3_reduction + fma_cycles + exp_cycles + rowsum_cycles + rowsum_reduction + scaling_cycles\n",
    "    return total_cycles\n",
    "\n",
    "def ret_ktile(bpe: int=2, tile_sizes:tuple = (128,128), gpu_name: str = \"mi300\"):\n",
    "    for _val in tilek_sizes:\n",
    "        if ((tile_sizes[0]+tile_sizes[1])*bpe*_val <=  (system_config[gpu_name]['lds_cap']*1024)//2):\n",
    "            return _val\n",
    "        else:\n",
    "            continue\n",
    "    assert(0)\n",
    "\n",
    "def find_bestfaperformance(q_len: int, k_len: int, attn_dim: int, num_heads: int , bpe: int = 2, gpu_name: \"mi300\"):\n",
    "    \n",
    "    #2D shard per GPU for num_heads x num_query\n",
    "    num_cus = system_config[gpu_name]['num_cus'] // num_heads\n",
    "    tile_size= q_len // num_cus\n",
    "    if tile_size >= 128:\n",
    "        tile_size = 128\n",
    "    elif tile_size >= 64:\n",
    "        tile_size = 64\n",
    "    elif tile_size >= 32:\n",
    "        tile_size = 32\n",
    "    elif tile_size >= 16:\n",
    "        tile_size = 16\n",
    "    else:\n",
    "        tile_size = q_len\n",
    "        \n",
    "    q_tiles = q_len//tile_size\n",
    "    num_fatiles = q_tiles * num_heads\n",
    "    fatiles_percu = num_fatiles // system_config[gpu_name]['num_cus']\n",
    "    \n",
    "    qtile_size = tile_size\n",
    "    #LDS memory to hold K/V elements; Q tile is persistent in register\n",
    "    #64KB LDS size 2 WG(S) holding K elements  \n",
    "    kvtile_size = 64*1024 // (2*attn_dim * bpe)\n",
    "    \n",
    "    eff = flashattn_eff(fatiles_percu,bpe,attn_dim,qtile_size,kvtile_size,kvtile_size,gpu_name)\n",
    "    return eff\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def find_bestgemmperformance(bpe: int =2, M: int = None, N: int =None, K:int =None, gpu_name: str = \"mi300\"):\n",
    "    assert(K!=None)\n",
    "    assert(M!=None)\n",
    "    assert(N!=None)\n",
    "    \n",
    "    _efficiency_table = []\n",
    "    for tile_size in tile_sizes:\n",
    "        ktile = ret_ktile(bpe,tile_size,gpu_name)\n",
    "        _effmetrics = {}\n",
    "        _effmetrics['tile_size'] = tile_size\n",
    "        _effmetrics['eff'] = gemm_eff(bpe,tile_size[0],tile_size[1],ktile,K,gpu_name)\n",
    "        _effmetrics['granu_loss'] = granu_loss(gpu_name,M,N,tile_size[0],tile_size[1])\n",
    "        _efficiency_table.append(_effmetrics)\n",
    "        \n",
    "    pick_winner = []    \n",
    "    for idx,_item in enumerate(_efficiency_table):\n",
    "        metrics = _item[idx]\n",
    "        pick_winner.append(metrics['eff']*metrics['granu_loss'])\n",
    "    \n",
    "    winner_idx = np.argmax(pick_winner)\n",
    "    return _efficiency_table[winner_idx]\n",
    "\n",
    "def granu_loss(gpu_name: str = 'mi300', gemm_m : int = 1024, gemm_n : int = 1024, mtile:int =128, ntile:int =128):\n",
    "    \n",
    "    num_cus = system_config[gpu_name]\n",
    "    mtile = tile_size[0]\n",
    "    ntile = tile_size[1]\n",
    "    m_tiles = gemm_m//mtile\n",
    "    n_tiles = gemm_n//ntile\n",
    "    tiles_per_cu = m_tiles*n_tiles // num_cus\n",
    "    return tiles_per_cu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peak memory assertions for various system Components\n",
    "# for running  large models using single GPU + CPU memory + disk CAche\n",
    "\n",
    "def GPU_PeakMem(Phase : MODELPHASE = MODELPHASE.TOKEN.value, ModelConfig: dict = None, probMem: dict = None, batch_size: int =1, sysCfg: dict() = None) -> bool:\n",
    " \n",
    "    \"\"\" Provide GPU peak memory (in elements) required for given model size & model phase\"\"\"\n",
    "    \n",
    "    if ProbMem is None:\n",
    "        raise ValueError(\"ProbMem is None\")\n",
    "\n",
    "    dim1 = ModelConfig[\"hidden_dim1\"]\n",
    "    dim2 = ModelConfig[\"hidden_dim2\"]\n",
    "    seqlen = ModelConfig[\"max_seq_len\"] \n",
    "    outToken = ModelConfig[\"out_tokens\"]\n",
    "    numLayers = ModelConfig[\"num_layers\"]\n",
    "    numHeads = ModelConfig[\"num_heads\"]\n",
    "    \n",
    "    GpuMem_resident_layer = probMem[\"WeightsGpu\"] * 4*pow(dim1,2) + 2*dim1*dim2  + probMem[\"KVCacheGpu\"]*2*(seqlen + outToken)*dim1\n",
    "                                    \n",
    "    resident_act = ProbMem[\"ActGpu\"]*batch_size*seqlen*dim1 if Phase == MODELPHASE.PREFILL.value else ProbMem[\"ActGpu\"]*batch_size*1*dim1\n",
    "    GpuMem_resident = GpuMem_resident_layer * numLayers + resident_act\n",
    "    \n",
    "    #Working memory for each op\n",
    "    #QKV tensor matrices seqlen x dim1 [3] + input seqlen * dim1  \n",
    "    #for Token generation seqlen = 1\n",
    "    QKV = batch_size*(4*seqlen*dim1) if Phase == MODELPHASE.TOKEN.value else batch_size*(4*dim1)\n",
    "    if Phase == MODELPHASE.PREFILL.value:\n",
    "        #QK inputs + attention outputs\n",
    "        QK_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(seqlen*dim1 + seqlen*seqlen*numHeads + seqlen*dim1)\n",
    "        QKV_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(seqlen*dim1 + seqlen*seqlen*numHeads + seqlen*dim1)\n",
    "    else:\n",
    "        #QK inputs Q seqlen =1 K seqlen (kv cache) \n",
    "        QK_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(dim1 + 1*(seqlen+outToken)*numHeads + (seqlen+outToken)*dim1)\n",
    "        QKV_attention = ProbMem[\"KVCacheGpu\"]*batch_size*(1*dim1 + 1*(seqlen+outToken)*numHeads + (seqlen+outToken)*dim1)   \n",
    "        \n",
    "    Embed = batch_size*(seqlen*dim1*2) if Phase == MODELPHASE.PREFILL.value else  batch_size * (2*dim1)\n",
    "    MLP1 = batch_size*(seqlen*dim1 + seqlen*dim2) if Phase == MODELPHASE.PREFILL.value else batch_size*(1*dim1 + 1*dim2)\n",
    "    MLP2 = batch_size*(seqlen*(dim1+dim2)) if Phase == MODELPHASE.PREFILL.value else batch_size*(1*dim1 + 1*dim2)\n",
    "    \n",
    "    #Total working memory by adding xfered weights stored higher hierarchy \n",
    "    # activation stored in hierachy memory\n",
    "    # max(all the ops in one layer)\n",
    "    workingMem = (1 - probMem[\"WeightsGpu\"])*(4*pow(dim1,1) + 2*dim1*dim2) +\\\n",
    "                (1 - probMem[\"ActGpu\"])*batch_size*seqlen*dim1 + \\\n",
    "                 max(QKV,QK_attention,QKV_attention,Embed,MLP1,MLP2)\n",
    "                 \n",
    "    PeakMemory = workingMem + GpuMem_resident\n",
    "    assert(PeakMemory < sysCfg['gpuMemCapacity'])\n",
    "    return True\n",
    "    \n",
    "\n",
    "\n",
    "def  CPU_PeakMem(Phase : MODELPHASE = MODELPHASE.TOKEN.value, \n",
    "                 ModelConfig: dict = None, probMem: dict = None, \n",
    "                 batch_size: int =1, \n",
    "                 sysCfg: dict = None) -> bool:        \n",
    "    \n",
    "    \"\"\" \n",
    "    Provide CPU peak memory (in elements) required for given model size & model phase\n",
    "    \"\"\"\n",
    "    \n",
    "    if ProbMem is None:\n",
    "        raise ValueError(\"ProbMem is None\")\n",
    "    \n",
    "    dim1 = ModelConfig[\"hidden_dim1\"]\n",
    "    dim2 = ModelConfig[\"hidden_dim2\"]\n",
    "    seqlen = ModelConfig[\"max_seq_len\"] \n",
    "    outToken = ModelConfig[\"out_tokens\"]\n",
    "    numLayers = ModelConfig[\"num_layers\"]\n",
    "    #numHeads = ModelConfig[\"num_heads\"]\n",
    "    \n",
    "    CpuMem_resident_layer = probMem[\"WeightsCpu\"] * (4*pow(dim1,2)) + 2*dim1*dim2  + probMem[\"KVCacheCpu\"]*2*(seqlen + outToken)*dim1\n",
    "    resident_act = ProbMem[\"ActCpu\"]*batch_size*seqlen*dim1 if Phase == MODELPHASE.PREFILL.value else ProbMem[\"ActGpu\"]*batch_size*1*dim1\n",
    "    CpuMem_resident = CpuMem_resident_layer * numLayers + resident_act\n",
    "    \n",
    "    #working memory stored in disk through CPU\n",
    "    #Total working memory by adding xfered weights stored higher hierarchy \n",
    "    # activation stored in hierachy memory\n",
    "    # max(all the ops in one layer)\n",
    "    workingMem = (1 - probMem[\"WeightsCpu\"])*(4*pow(dim1,1) + 2*dim1*dim2) +\\\n",
    "                (1 - probMem[\"ActCpu\"])*batch_size*seqlen*dim1 \n",
    "    \n",
    "    PeakMemory = workingMem + CpuMem_resident\n",
    "    assert(PeakMemory < sysCfg['gpuMemCapacity'])\n",
    "    return True\n",
    "    \n",
    "def  disk_PeakMem(Phase : MODELPHASE = MODELPHASE.TOKEN.value, \n",
    "                  ModelConfig: dict = None, \n",
    "                  probMem: dict = None, \n",
    "                  batch_size: int =1, \n",
    "                  sysCfg: dict = None) -> bool:\n",
    "    \n",
    "    if ProbMem is None:\n",
    "        raise ValueError(\"ProbMem is None\")\n",
    "    dim1 = ModelConfig[\"hidden_dim1\"]\n",
    "    dim2 = ModelConfig[\"hidden_dim2\"]\n",
    "    seqlen = ModelConfig[\"max_seq_len\"] \n",
    "    outToken = ModelConfig[\"out_tokens\"]\n",
    "    numLayers = ModelConfig[\"num_layers\"]\n",
    "    \n",
    "    disk_resident_layer = probMem[\"WeightsDisk\"] * (4*pow(dim1,2)) + 2*dim1*dim2  + probMem[\"KVCacheDisk\"]*2*(seqlen + outToken)*dim1 \n",
    "    resident_act = ProbMem[\"ActDisk\"]*batch_size*seqlen*dim1 \n",
    "    PeakMemory = disk_resident_layer * numLayers + resident_act\n",
    "\n",
    "    assert(PeakMemory < sysCfg['gpuMemCapacity'])\n",
    "    return True\n",
    "    \n",
    "def activationmem_per_layer(batch_size:int, seq_len: int, model_dim1: int, model_dim2: int, FA: bool= True):\n",
    "    #input seqlen*batch_size*model_dim \n",
    "    #Q,K,V tensor(s) = 3*batch_size*seq_len*model_dim\n",
    "    #output projection = seqlen*batch_size*model_dim\n",
    "    #MLP1 = seqlen*model_dim1*\n",
    "    #MLP2 = seqlen*model_dim\n",
    "    #layernorm = seqlen*batch_size*model_dim \n",
    "    # max (above all)\n",
    "    # IF FA == True, no QK output, no dropout,..\n",
    "    input = batch_size*seq_len*model_dim1\n",
    "    QKV_tensor = batch_size*seq_len*model_dim1*3\n",
    "    attn_output = batch_size*seq_len*model_dim1\n",
    "    MLP1_output = batch_size*seq_len*model_dim2\n",
    "    MLP2_output = batch_size*seq_len*model_dim1\n",
    "    layer_norm = batch_size*seq_len*model_dim1\n",
    "    \n",
    "    activation_mem = max(input,QKV_tensor,attn_output,MLP1_output,MLP2_output,layer_norm)\n",
    "    return activation_mem\n",
    "\n",
    "\n",
    "def kvcache_per_layer(Phase: MODELPHASE = MODELPHASE.PREFILL.value,\n",
    "                      batch_size:int =1, \n",
    "                      outTokenSize: int=1, \n",
    "                      seq_len:int = 1, \n",
    "                      hidden_dim:int =1):\n",
    "    \n",
    "    if Phase == MODELPHASE.PREFILL.Value:\n",
    "       return 2*(seq_len*batch_size*hidden_dim)\n",
    "    else:   \n",
    "       return (2*(seq_len+outTokenSize)*batch_size*hidden_dim)\n",
    "   \n",
    "        \n",
    "def model_parameters(model_name:str, component:str):\n",
    "    \n",
    "    if model_name in MODEL_CLASSES:\n",
    "        modelDict = MODEL_CLASSES[model_name]\n",
    "    else:\n",
    "        raise ValueError(f\" Model {model_name} not supported yet\")    \n",
    "    if component == MODELCOMPONENTS.Weights.value:\n",
    "        ## 4*hidden_dim**2 + 2*model_dim*hidden_dim + 2*hidden_dim * num_layers + 2*(seqlen + vocab_size)*model_dim  \n",
    "        num_parameters_ = (4*pow(modelDict[\"model_dim\"],2) + 2*modelDict[\"model_dim\"]*modelDict[\"hidden_dim\"] + 2*modelDict[\"model_dim\"]) *modelDict[\"num_layers\"]\n",
    "        num_parameters_ +=  (modelDict[\"seq_len\"]+modelDict[\"vocab_size\"])*modelDict[\"model_dim\"]\n",
    "        return num_parameters_\n",
    "    if component == MODELCOMPONENTS.Activation.value:\n",
    "        activation_ = activationmem_per_layer(batch_size=modelDict[\"batch_size\"], seq_len = modelDict[\"seq_len\"],model_dim1=modelDict[\"model_dim\"],model_dim2=modelDict[\"hidden_dim\"])\n",
    "        return activation_\n",
    "        \n",
    "    if component == MODELCOMPONENTS.KVcache.value:\n",
    "        kvCache_ = kvcache_per_layer(batch_size=modelDict[\"batch_size\"], seq_len = modelDict[\"seq_len\"], hidden_dim=modelDict[\"hidden_dim\"], outTokenSize=modelDict[\"out_tokens\"])\n",
    "        return kvCache_\n",
    "        \n",
    "    raise ValueError(f\" not supported {component} yet\")\n",
    "\n",
    "def qkv_projection_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value):\n",
    "    return batch_size*3*2*modelCfg['max_seq_len']*modelCfg['model_dim']*modelCfg['model_dim']\n",
    "\n",
    "def MLP_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value):\n",
    "    return 2*2*batch_size*modelCfg['max_seq_len']*modelCfg['model_dim']*modelCfg['hidden_dim']\n",
    "\n",
    "def out_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value):\n",
    "    return batch_size*2*modelCfg['max_seq_len']*modelCfg['model_dim']*modelCfg['model_dim']\n",
    "\n",
    "def flashAttention_compute(modelCfg: dict, batch_size: int =1, Phase: MODELPHASE = MODELPHASE.TOKEN.value, CMasking:bool = True):\n",
    "    if Phase == MODELPHASE.PREFILL.Value:       \n",
    "        FA = 2*2*batch_size*modelCfg['max_seq_len']*modelCfg['max_seq_len']*modelCfg['model_dim']\n",
    "        FA = FA//2 if CMasking == True else FA\n",
    "        return FA\n",
    "    else:\n",
    "        FA = 2*2*batch_size*modelCfg['max_seq_len']*modelCfg['model_dim']\n",
    "        FA = FA//2 if CMasking == True else FA\n",
    "        return FA\n",
    "\n",
    "def GPUPeakMem(model_name:str):\n",
    "    weights= model_parameters(model_name,\"weights\")\n",
    "    activation = model_parameters(model_name,\"activation\")\n",
    "    kvcache = model_parameters(model_name,\"KVcache\")\n",
    "    \n",
    "    peakMem = weights+activation+kvcache\n",
    "    return peakMem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MESH shard (x,y)\n",
    "\n",
    "# ----------------\n",
    "# |0 | 1 | 2 | 3 |\n",
    "# |4 | 5 | 6 | 7 | \n",
    "# ----------------\n",
    "\n",
    "#pipeline parallelism = X dimension = 4 GPU(S)  layers_per_rank = layers/num_ranks  \n",
    "#model parallelism    = Y dimension = 2 GPU(s)  tensors_per_rank  = hidden_dim/num_ranks model_dim/num_ranks  heads_per_rank = attn_heads/num_ranks\n",
    "# data_parallelism    = X dimension = 4 GPU(s)  batchsize_per_rank = batch_size/num_ranks\n",
    "\n",
    "\n",
    "\n",
    "#2D sharding for 8 GPU(s)\n",
    "\n",
    "mesh_shard = (4,2)  # (x,y)\n",
    "mesh_dict = {'X': mesh_shard[0], 'Y': mesh_shard[1]}\n",
    "# weights: hidden_dim, model-dim = Y\n",
    "# activation: batch and model_dim  = X\n",
    "\n",
    "#B = batchsize\n",
    "#N = number of heads\n",
    "#M = model-dimension\n",
    "#S = sequence_len\n",
    "#H = hidden_dimension\n",
    "embedding_shard_BSM = {'batch_size' : 'X', 'max_seq_len' : '_', 'model_dim': \"_'\"}\n",
    "activation_shard_BSM = {'batch_size'  : 'X', 'max_seq_len' : '_', 'model_dim' : '_'}\n",
    "activation_shard_BSND = {'batch_size' : 'X', 'max_seq_len': '_', 'attn_heads': 'Y', 'attn_dim' : '_'}\n",
    "activation_shard_BSH = {'batch_size'  : 'X', 'max_seq_len' : '_', 'hidden_dim' : 'Y'}\n",
    "activation_shard_BNSS = {'batch_size' : 'X', 'attn_heads' : 'Y', 'max_seq_len' : '_'}\n",
    "\n",
    "from dataclasses import dataclass\n",
    "#first key is summation dimension second key is free0/free1 dimension\n",
    "weights_shard_MM  = {'model_dim'  : '_', 'model_dim1' : '_'}\n",
    "weights_shard_MND = {'model_dim'  : '_', 'attn_heads'  : 'Y', 'attn_dim' : '_'} \n",
    "weights_shard_NDM = {'attn_heads' : 'Y', 'attn_dim'  : '_', 'model_dim'  : 'X'}\n",
    "weights_shard_MH  = {'model_dim'  : 'X', 'hidden_dim' : 'Y'}\n",
    "weights_shard_HM  = {'model_dim'  : 'X', 'hidden_dim' : 'Y'}\n",
    "embedding_weight =  {'vocab_size'  : '_', 'model_dim' : '_'}\n",
    "\n",
    "@dataclass\n",
    "class transformer_2dshard:\n",
    "    embedding_activations = {'input0' : embedding_shard_BSM, 'output' : embedding_shard_BSM}\n",
    "    qkv_activations = {'input0' : activation_shard_BSM , 'input1' : weights_shard_MM, 'output' : activation_shard_BSND}    # weights M=8192  N=64 D = 128   activation : B=1x2048x8192  (1x2048x128, 64) \n",
    "    flash_attention_qk = {'input0' : activation_shard_BSND , 'input1' : activation_shard_BSND, 'output' : activation_shard_BNSS}\n",
    "    flash_attention_sv = {'input0' : activation_shard_BSND , 'input1' : activation_shard_BNSS, 'output' : activation_shard_BSND}\n",
    "    out_projection =  {'input0' : activation_shard_BSND , 'input1' :  weights_shard_NDM , 'output' : activation_shard_BSM}\n",
    "    #FIXME sequence_len sharding \n",
    "    layer_norm = {'input' : activation_shard_BSM , 'output' : activation_shard_BSM}\n",
    "    mlp_0 = {'input0': activation_shard_BSM, 'input1' : weights_shard_MH, 'output' : activation_shard_BSH}\n",
    "    gelu  = {'input': activation_shard_BSH, 'output' : activation_shard_BSH}\n",
    "    mlp_1 = {'input1': activation_shard_BSH, 'input1' : weights_shard_HM, 'output' : activation_shard_BSM}\n",
    "    \n",
    "    def __init__(self,model_name:str = \"llama_65B\",\n",
    "                 bpe : int = 2, \n",
    "                 num_gpus : int =1, shard_topology : tuple = None, \n",
    "                 parallelism_cfg: dict = None, \n",
    "                 sysCfg: system_config = None,\n",
    "                 tileSizes: list[tuple] = None):\n",
    "        \n",
    "        self.bpe = bpe\n",
    "        if model_name in MODEL_CLASSES.keys():\n",
    "            self.modelParameters = MODEL_CLASSES[model_name]  \n",
    "            self.model_name = model_name\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model\")\n",
    "        self.numGPUs = num_gpus\n",
    "        self.shard_topology = shard_topology\n",
    "        self.systemCfg = sysCfg\n",
    "        self.fp16rate = self.systemCfg['mi300']['num_cus']*self.systemCfg['mi300']['fp16_rate']\n",
    "        if parallelism_cfg is None:\n",
    "            self.parallelismDict = dict()\n",
    "            self.parallelismDict['tensorParallelism'] = True\n",
    "            self.parallelismDict['dataParallelism'] = False\n",
    "            self.parallelismDict['pipelineParallelism'] = True\n",
    "        else:\n",
    "            self.parallelismDict = parallelism_cfg   \n",
    "        \n",
    "        self.tileSizeChoices = tileSizes\n",
    "        self.max_seq_len = self.modelParameters['max_seq_len']\n",
    "        \n",
    "        #support 2D mesh for sharding \n",
    "        assert (not(self.parallelismDict['tensorParallelism'] and self.parallelismDict['dataParallelism'] and self.parallelismDict['pipelineParallelism']))                                                                                                                                                                                                                                               \n",
    "        if self.parallelismDict['pipelineParallelism'] and not self.parallelismDict['dataParallelism']: \n",
    "            self.pipeline_num_ranks = self.shard_topology[0]\n",
    "            self.layers_per_rank  = self.systemCfg['num_layers'] // self.pipeline_num_ranks\n",
    "            self.num_microbatches = self.modelParameters['batch_size']//self.modelParameters['microbatch_size']\n",
    "            \n",
    "        #data parallelism enabled when pipeline parallelism \n",
    "        #batch_Size is global batch size\n",
    "        if self.pipeline_num_ranks == 1:\n",
    "            self.data_num_ranks = self.shard_topology[0] \n",
    "            self.micro_batchsize = self.modelParameters['batch_size']//self.shard_topology[0]\n",
    "            \n",
    "        self.tensor_num_ranks = self.shard_topology[1]\n",
    "        \n",
    "        #self.input2dim = {'batch_size' : 'X', \n",
    "        #                  'model_dim' : 'Y', \n",
    "        #                  'hidden_dim' : 'Y', \n",
    "        #                  'attn_heads' : 'Y'}\n",
    "        \n",
    "        #self.dim2Input = {'X' : ['batch_size','model_dim'],\n",
    "        #                  'Y' : ['attn_heads','hidden_dim']}\n",
    "    def peakmem_check(self):\n",
    "        memUsage = GPUPeakMem(self.model_name)\n",
    "        #pipeline parallelism takes batch_size dimension\n",
    "        memCapacity = self.systemCfg['hbm_cap']\n",
    "        if self.parallelismDict['pipelineParallelism'] and not self.parallelismDict['dataParallelism']:\n",
    "            memUsage = memUsage//self.shard_topology[0]\n",
    "        assert(memUsage <= memCapacity)\n",
    "        \n",
    "    def recv_payloadsize(self):\n",
    "        return self.modelParameters['batch_size'] * self.modelParameters['model_dim'] * self.modelParameters['max_seq_len']\n",
    "    \n",
    "    def layer_ops(self,seq_len: int):\n",
    "        \n",
    "        #qkv_projections\n",
    "        act_tensor = {}\n",
    "        wt_tensor = {}\n",
    "        for key, _ in self.qkv_activations['input0'].items():\n",
    "            if key == 'batch_size':\n",
    "                act_tensor['batch_size'] = self.micro_batchsize\n",
    "            elif key == 'max_seq_len':\n",
    "                act_tensor['M'] = seq_len\n",
    "            elif key == 'model_dim':\n",
    "                act_tensor['K'] = self.modelParameters['model_dim'] \n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSM tensor\")\n",
    "                \n",
    "        for key, value in self.qkv_activations['input1'].items():\n",
    "            if key == 'model_dim':\n",
    "                wt_tensor['K'] = self.modelParameters['model_dim']\n",
    "            elif key == 'model_dim1':\n",
    "                wt_tensor['N'] = self.modelParameters['model_dim']\n",
    "                if value != '_':\n",
    "                    wt_tensor['N'] = wt_tensor['N']//self.tensor_num_ranks\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSM tensor\")        \n",
    " \n",
    "        print(f\"QKV_Projection:: GEMM problem sizes M={act_tensor['M']} N= {3*wt_tensor['N']} K= {act_tensor['K']}\")\n",
    "        #returns efficiency\n",
    "        efficiency = find_bestgemmperformance(self.bpe,act_tensor['M'],3*wt_tensor['N'],act_tensor['K'],\"mi300\")\n",
    "        qkv_cycles = ((2*act_tensor['M'])*3*wt_tensor['N']*act_tensor['K'])//(efficiency['eff']*self.systemCfg['mi300']['num_cus']*self.systemCfg['mi300']['fp16_rate'])\n",
    "      \n",
    "        #flash attention \n",
    "        #calculate num_heads per GPU\n",
    "        #case 1: if heads >32 split heads in 'Y' dimension\n",
    "        q_tensor = {}\n",
    "        k_tensor = {}\n",
    "        v_tensor = {}\n",
    "\n",
    "        for key, value in self.flash_attention_qk['input0'].items():\n",
    "            if key == 'batch_size':\n",
    "                q_tensor['batch_size'] = self.micro_batchsize\n",
    "            elif key == 'max_seq_len':\n",
    "                q_tensor['M'] = seq_len\n",
    "            elif key == 'attn_dim':\n",
    "                q_tensor['K'] = self.modelParameters['model_dim'] // self.modelParameters['num_heads']\n",
    "            elif key == 'attn_heads':\n",
    "                heads_num_ranks = 1 if value == '_' else mesh_dict[value]\n",
    "                q_tensor['h']  = self.modelParameters['num_heads']\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSND tensor\")\n",
    "        \n",
    "        for key, _ in self.flash_attention_qk['input1'].items():\n",
    "            if key == 'batch_size':\n",
    "                k_tensor['batch_size'] = self.micro_batchsize\n",
    "            elif key == 'max_seq_len':\n",
    "                k_tensor['M'] = seq_len\n",
    "            elif key == 'attn_dim':\n",
    "                k_tensor['K'] = self.modelParameters['model_dim'] // self.modelParameters['num_heads']\n",
    "            elif key == 'attn_heads':\n",
    "                k_tensor['h']  = self.modelParameters['num_heads']\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSND tensor\")\n",
    "                \n",
    "\n",
    "        #shard heads in 'Y' dimension     \n",
    "        q_tensor['h'] = q_tensor['h']//heads_num_ranks\n",
    "        \n",
    "        fa_eff = find_bestfaperformance(q_tensor['M'],k_tensor['N'],q_tensor['K'],q_tensor['h'],self.bpe,\"mi300\")\n",
    "        fa_flops = 2*(q_tensor['h']*(q_tensor['M']*q_tensor['K']*k_tensor['N']  + v_tensor['K']*q_tensor['M']*q_tensor['K']))\n",
    "        fa_cycles = fa_flops//(self.fp16rate*fa_eff)\n",
    "        \n",
    "        #attention result projection output to [B,S,M]\n",
    "        #input [B,S,N,D] x [N,D,M] -> [B,S,M]\n",
    "        #shard on model-dimension\n",
    "        #gather for reduction\n",
    "        o_tensor = {}\n",
    "        wo_tensor = {}\n",
    "        \n",
    "        for key, value in self.out_projection['input0'].items():\n",
    "            if key == 'batch_size':\n",
    "                o_tensor['batch_size'] = self.micro_batchsize\n",
    "            elif key == 'max_seq_len':\n",
    "                o_tensor['M'] = seq_len\n",
    "            elif key == 'attn_dim':\n",
    "                o_tensor['K'] = self.modelParameters['model_dim'] // self.modelParameters['num_heads']\n",
    "            elif key == 'attn_heads':\n",
    "                heads_num_ranks = 1 if value == '_' else mesh_dict[value]\n",
    "                o_tensor['h']  = self.modelParameters['num_heads']\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSND tensor\")\n",
    "            \n",
    "        for key, value in self.out_projection['input1'].items():\n",
    "            if key == 'model_dim':\n",
    "                wo_tensor['N'] = self.modelParameters['model_dim']\n",
    "                tensor_num_ranks  = 1  if value == '_' else self.tensor_num_ranks\n",
    "            elif key == 'attn_dim':\n",
    "                wo_tensor['K'] = self.modelParameters['model_dim'] // self.modelParameters['num_heads']\n",
    "            elif key == 'attn_heads':\n",
    "                heads_num_ranks = 1 if value == '_' else mesh_dict[value]\n",
    "                wo_tensor['h']  = self.modelParameters['num_heads']\n",
    "            else:\n",
    "                raise ValueError(f\"unknown key in activation_shard_BSND tensor\")    \n",
    "        \n",
    "        #gather heads before Out dimension\n",
    "        #payload = num_heads * batch_size * seq_len * attn_dim (B,S,N,D)\n",
    "        #FIXME communication & reduction \n",
    "        #global reduction through gather and reduction\n",
    "        if heads_num_ranks > 1: \n",
    "           payload_size = o_tensor['h']*o_tensor['M']*wo_tensor['N']\n",
    "           all_reduce_cycles = 10\n",
    "        else:\n",
    "           all_reduce_cycles = 0\n",
    "           \n",
    "        wo_tensor['N'] = wo_tensor['N'] // tensor_num_ranks \n",
    "        efficiency = find_bestgemmperformance(self.bpe,o_tensor['M'],wo_tensor['N'],o_tensor['K']*o_tensor['h'],\"mi300\")\n",
    "        out_cycles = (2*o_tensor['M']*wo_tensor['N']*o_tensor['K']*o_tensor['h'])//(efficiency['eff']*self.systemCfg['mi300']['num_cus']*self.systemCfg['mi300']['fp16_rate']) \n",
    "        \n",
    "        #global reduction through gather and reduction\n",
    "        #MLP0 w = MxHy   act = BSMx\n",
    "        #ALL-GATHER Mx ->M \n",
    "        #GELU\n",
    "        #MLP1 w = HyMx act = BSHy\n",
    "        #ALL-GATHER\n",
    "        #MX -> M\n",
    "        #output = BSHy HyM\n",
    "        #       = BSM(partials)\n",
    "        #       = BSMy (reduce & scatter)       \n",
    "        \n",
    "        \n",
    "        fa_latency = 1\n",
    "        norm_latency  = 1\n",
    "        gather_latency = 1\n",
    "        ff_latency = 1\n",
    "        reduction_latency = 1\n",
    "        \n",
    "        return 1\n",
    "        \n",
    "    def prefill_forward(self):\n",
    "        \n",
    "    #rank[0] has additional layer of embedding beside other layers\n",
    "        #rank[n-1] has additional layers of norm()\n",
    "        #ignore embedding and norm() for now\n",
    "        \n",
    "        num_ranks = self.shard_topology[0]  if self.parallelismDict['pipelineParallelism'] and not self.parallelismDict['dataParallelism'] else 1\n",
    "        layers_per_rank  = self.systemCfg['num_layers'] // num_ranks\n",
    "        \n",
    "        #every rank has send and recv communication along with other primitives for tensor  parallelism\n",
    "        payload_size = self.modelParameters['']\n",
    "        # payload_size // (lank_BW * efficiency)\n",
    "        _recv_latency = self.recv_payloadsize()\n",
    "        layer_latency = self.layer_ops()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #rank{x}\n",
    "        #rank{n-1}\n",
    "\n",
    "#all_gather \n",
    "#scatter_gather\n",
    "#all_reduce\n",
    "\n",
    "#pipeline/model paralllelism  num_layers/num_ranks   \n",
    "#tensor parallelism = MLP/MOE \n",
    "#TODO\n",
    "# fix the compute graph with sharding\n",
    "# calculate each layer time for prefill & generation \n",
    "# caclculate inference time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "# language model FLOPS \n",
    "# 3 * QKV 1 * projection + 2MLP  * number_of_layers\n",
    "d_model = 1024\n",
    "per_layer_flops = 2*(3*pow(d_model,2) + pow(d_model,2)*4*2 + pow(d_model,2))\n",
    "per_layer_flops = 2*12*pow(d_model,2)\n",
    "\n",
    "kv_cache_flops = 2*2*pow(d_model,2)\n",
    "\n",
    "mi300_fp16_flops = 304*2048/1e3  #Tflops\n",
    "mi300_BW = 4.3e12 \n",
    "mi300_capacity = 192e9\n",
    "bpe=2\n",
    "\n",
    "#52B model\n",
    "num_layers = 64\n",
    "d_model = 8192\n",
    "num_head = 64\n",
    "model_parameters = 52e9\n",
    "\n",
    "\n",
    "#work request parameters\n",
    "seqlen : List(int) = [1024,2048,4096,8192,16384]\n",
    "batch_size = np.arange(1,512,8,dtype=int)\n",
    "\n",
    "#parallelism parameters\n",
    "N = 4    # number of accelerators\n",
    "\n",
    "\n",
    "model_size = model_parameters*bpe\n",
    "kv_cache_size = 2*bpe*(seqlen*d_model*num_layers)\n",
    "token_size = mi300_capacity - (model_size+kv_cache_size)\n",
    "\n",
    "#pipeline parallelism\n",
    "#split layers among N accelerators\n",
    "\n",
    "#model parallelism (tensor parallelism)\n",
    "#attention - split #heads N accelerators\n",
    "#MLP - split tensors(weight) N accelerators\n",
    "# num_heads = h  hd = d_model/h We\n",
    "# Wq,Wk,Wv  (num_heads,d_modelxhd) \n",
    "# each layer algorithm\n",
    "#  S= Q@K => P = softmax(S) => P = P @ V => O = P @ W_o   \n",
    "# send d_model/N to N-1 accelerators \n",
    "# receive d_model/N from N-1 accelerators\n",
    "# add d_model/N vectors from N-1 accelerators \n",
    "\n",
    "#MLP weights 4*d_model / N split\n",
    "#MLP1 = d_model @ 4*d_model/N => 4*d_model/N @ d_model \n",
    "# send 4*d_model/N to N-1 accelerators\n",
    "# receive 4*d_model/N from accelerators\n",
    "# in total each layer require 4*communication ops\n",
    "\n",
    "#batch=1 case\n",
    "# mem_time (math is boud by memory BW) = bpe*model_parameters/ (N*mi300_bw)\n",
    "# communication = 4*num_layers*8us (latency due to BS=1)\n",
    "\n",
    "#batch = large\n",
    "# compute = 2*P/(N*)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict\n",
    "\n",
    "seqlen : List[int] = [1024,2048,4096,8192,16384]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
