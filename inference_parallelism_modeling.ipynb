{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##inferencing modeling for 1.8T parameter\n",
    "\n",
    "#what is performance metric to measure model performance\n",
    "# throught put  n tokens/s \n",
    "# latency   1 / micro-second\n",
    "\n",
    "#problem statement\n",
    "# maximizes GPU utilization (tokens per second per GPU) through batching different user requests without incurring additional costs\n",
    "# maximizes user interactivity (repsonse time = time to wait for response, measured tokens per second per user)\n",
    "\n",
    "#inference context 128K-1M\n",
    "#prefill context 8192\n",
    "\n",
    "#number of GPU/s per Node\n",
    "num_GPU_per_node = 8\n",
    "\n",
    "#nvidia\n",
    "mem_per_stack = 24*1e9\n",
    "num_stacks = 8\n",
    "chip_config_name = [\"b100\",\"b200\"]   # per GPU is two devices \n",
    "\n",
    "capacity_per_config_per_GPU = num_stacks * mem_per_stack\n",
    "\n",
    "#memory capacity requirement\n",
    "data_type = [\"fp16\",\"fp8\",\"fp4\"]\n",
    "\n",
    "#benchmark for inference \n",
    "# TTFT = time to first token  (user interactivity)\n",
    "# time per output token (tpot): time to generate an output token for each user  100 ms / token /user   -> 10 tokens/sec/user \n",
    "# latency = ttft + (tpot)*(total_output_tokens)\n",
    "# throughput = number of output tokens / second \n",
    "\n",
    "#metrics from profiling inference servicing\n",
    "# MEMBW.utilization (BWU) = (KV cache size + model parameter -size) / TPOT \n",
    "# TPOT = (KV cache size + model parameter size) / BWU\n",
    "\n",
    "\n",
    "#higher parallelism lowers BWU ??\n",
    "# for fixes batch-size, move lower amount of data in each GPU results into lower BW achieved\n",
    "\n",
    "#latency reduction through parallelism\n",
    "#higher GPU(s) with fixed batch-size offers insignificant reduction in latency due to lower memory chunks movement and communication overhead\n",
    "#larger batch should help alleviate this problem\n",
    "\n",
    "#increasing batch-size increases TTFT and TPOT \n",
    "\n",
    "#The formula to calculate KV cache size is\n",
    "#batch_size * seqlen * (d_model/n_heads) * n_layers * 2  * 2 (bytes per Float16) * n_kv_heads\n",
    "\n",
    "\n",
    "\n",
    "#legends\n",
    "\n",
    "# n - number of tokens\n",
    "# d - attention dimension\n",
    "# h_kv - number of kv heads\n",
    "# h_q  - number of q heads\n",
    "# p - total number of GPUs\n",
    "# l - number of layers\n",
    "# p_j  - parallelism degree for parallelism strategy j (TP,PP,EP,DP)\n",
    "# F_x  - Flops for x tensor or layer op\n",
    "# R_x  - bytes of read for x tensor or layer op\n",
    "# AI_x - Arithmetic intensity of layer x or tensor op x\n",
    "# C - chunk size\n",
    "# T - execution time\n",
    "# T_p  - prefill latency\n",
    "# T_d  - decode latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model parameters\n",
    "@dataclass\n",
    "class modelConfig:\n",
    "    \n",
    "    num_layers  : int = 120\n",
    "    hidden_dim  : int = 10752\n",
    "    vocab_size  : int = 100256\n",
    "    mlp_factor  : int = 4\n",
    "    num_layers  : int = 120\n",
    "    context_len : int = 16384\n",
    "    output_tokens : int = 1024\n",
    "    total_experts : int = 16\n",
    "    active_experts : int = 4\n",
    "    fsdp_enable   : int = 1\n",
    "    fsdp_factor : float = 0.15\n",
    "    batch_size : int = 16\n",
    "    num_microbatches : int = 4\n",
    "    wt_bpe: int = 2\n",
    "    act_bpe: int = 2\n",
    "    kv_bpe: int = 2\n",
    "    \n",
    "    def baselayer_params(self):\n",
    "        return 4 * self.hidden_dim * self.hidden_dim + 13 * self.hidden_dim\n",
    "\n",
    "    def model_base(self):\n",
    "        return  (4 * self.hidden_dim * self.hidden_dim + 13 * self.hidden_dim) * self.num_layers + (self.vocab_size + self.context_len) * self.hidden_dim\n",
    "    \n",
    "    def moe_params(self):\n",
    "        return ( self.num_layers * self.total_experts) * (2 * self.mlp_factor * self.hidden_dim * self.hidden_dim)\n",
    "    \n",
    "    def qkv_flops(self,seqlen):\n",
    "        #print(6 * seqlen * self.hidden_dim * self.hidden_dim)\n",
    "        return  6 * seqlen * self.hidden_dim * self.hidden_dim\n",
    "   \n",
    "    def qkv_payload(self,seqlen):\n",
    "        return  (seqlen*self.hidden_dim*self.act_bpe ,  3 * self.hidden_dim * self.hidden_dim * self.wt_bpe)\n",
    "   \n",
    "    def attn_flops(self,seqlen):\n",
    "       return 2 * 2 * seqlen * seqlen * self.hidden_dim\n",
    "   \n",
    "    def kvcache_payload(self,seqlen):\n",
    "        return (2*seqlen*self.hidden_dim*self.kv_bpe)\n",
    "   \n",
    "    def outproj_flops(self,seqlen):\n",
    "       return 2 * seqlen * self.hidden_dim * self.hidden_dim\n",
    "    #ln_flops = 8 * context_len * hidden_dim * hidden_factor * hidden_dim\n",
    "    \n",
    "    def outproj_payload(self,seqlen):\n",
    "        return  (seqlen*self.hidden_dim*self.act_bpe ,   self.hidden_dim * self.hidden_dim * self.wt_bpe)\n",
    "    \n",
    "    def rope_flops(self,seqlen):\n",
    "       return seqlen * self.hidden_dim  # rotary position matrix is pre-computed \n",
    "    \n",
    "    def rope_payload(self,seqlen):\n",
    "        return  (seqlen*self.hidden_dim*self.act_bpe)\n",
    "    \n",
    "    def moe_flops(self,seqlen):\n",
    "    #normal  distribution of 'num_tot_experts' of 4 * context_len\n",
    "        expert_capacity = self.active_experts * seqlen / self.total_experts\n",
    "        return 2 * 2 * expert_capacity * self.hidden_dim * self.mlp_factor * self.hidden_dim   # 2 FFN\n",
    "    \n",
    "    def moe_payload(self,seqlen):\n",
    "        expert_capacity = self.active_experts * seqlen / self.total_experts\n",
    "        return (2*expert_capacity * self.hidden_dim*self.act_bpe, 2*self.hidden_dim * self.hidden_dim * self.wt_bpe)\n",
    "    \n",
    "    #unembedding_flops = 2 * context_len * hidden_dim * hidden_dim\n",
    "    def layernorm_flops(self,seqlen):\n",
    "        return 2 * 6 * self.hidden_dim * seqlen\n",
    "    \n",
    "    def layernorm_payload(self,seqlen):\n",
    "        return seqlen * self.hidden_dim * self.act_bpe\n",
    "    \n",
    "    \n",
    "    def topk_argmax(self,seqlen):\n",
    "        return seqlen * self.active_experts * self.total_experts * (self.active_experts-1) * 6   # argmax , sorted max, historgram , atomic \n",
    "    \n",
    "    def moe_topk_softmax(self,l2_latency,seqlen):\n",
    "        ##algorithm\n",
    "        \n",
    "        ## gemm(seqlen,hidden_dim,total_experts)\n",
    "        ## softmax(seqlen,total_experts)\n",
    "        ## for topk_iter 0 to active_experts\n",
    "        ##   for expert_iter 0 to num_experts//num_waves\n",
    "        ##       for compare_iter 0 to topk_iter -1\n",
    "        ##           read top_k[]   # l2 latency once and data kept in l1 for each iteration of topk_iter  l2 latency \n",
    "        ##           v_cmp + v_mov = 8 cycles\n",
    "        ##       block_reduce(0) = 64\n",
    "        ##   write out indices and topk_val #l2_latency  = 256 cycles\n",
    "        ##   sync_threads()  = 64 cycles\n",
    "        \n",
    "        \n",
    "        topk_cycles  = seqlen * self.active_experts * (l2_latency + self.total_experts  * 8 + 128 + l2_latency)   # 1 read + 1 write\n",
    "        gemm_cycles  = seqlen * self.hidden_dim * 4 // l2_latency   # activation in l2  weights are amortized by 100% reuse\n",
    "        softmax_cycles = seqlen * self.active_experts  * 12 // 64  # 64 ops / cycles\n",
    "        return (gemm_cycles , softmax_cycles, topk_cycles)\n",
    "    \n",
    "    def layer_flops(self,seqlen):\n",
    "        flops = self.qkv_flops(seqlen) + self.attn_flops(seqlen) + self.outproj_flops(seqlen) + self.moe_flops(seqlen)\n",
    "        return flops\n",
    "    \n",
    "#per_layer_base = (4*hidden_dim*hidden_dim + 13*hidden_dim)\n",
    "#model_base = (4*hidden_dim*hidden_dim + 13*hidden_dim)*num_layers + (VOCAB_SIZE+context_len)*hidden_dim\n",
    "#moe_params = num_layers*num_tot_experts*(2*factor*hidden_dim*hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.47708672\n"
     ]
    }
   ],
   "source": [
    "#per_layer_base = 2*(4*hidden_dim*hidden_dim ) #+ 13*hidden_dim)\n",
    "#print(8*per_layer_base/1000/1000/1000)\n",
    "#moe = 2*16*8*hidden_dim*hidden_dim/\n",
    "#print(8*moe/1000/1000/1000)\n",
    "#print(33792e3/1024/1024)\n",
    "\n",
    "model = modelConfig()\n",
    "print(model.layer_flops(1)*model.num_layers/1000/1000/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GpuConfig:\n",
    "    fp16_flops: int  #flops/cycle/cu\n",
    "    num_cus : int \n",
    "    fp32_flops : int   #valu co-execution\n",
    "    fma_flops : int # flops/cycle/cu \n",
    "    trans_flops : int  \n",
    "    freq : float #Ghz\n",
    "    hbm_bw: float  #Gbytes/s   #stream \n",
    "    hbm_capacity: int  #Gbytes\n",
    "    gpus_per_node : int \n",
    "    number_of_nodes : int \n",
    "    l2_latency : int    #in clocks\n",
    "    hbm_latency : int   #in clocks\n",
    "    nic_cards : int\n",
    "    intracommEff : float\n",
    "    intercommEff : float\n",
    "    topology : str  \n",
    "    intra_bw : float      #Gbytes/s  unidirectional per node 7 links/node\n",
    "    inter_bw : float      #Gbytes/s  #nic(s) card bw per node \n",
    "    intra_latency : int\n",
    "    inter_latency : int\n",
    "    reduction_bw : dict    #import tables for variouse sizes\n",
    "    gather_bw : dict\n",
    "    scatter_bw : dict\n",
    "    all2all_bw : dict\n",
    "    \n",
    "    def f16flops(self):\n",
    "        #print(f\"f16_flops = {self.fp16_flops * self.num_cus * self.freq}\")\n",
    "        return self.fp16_flops * self.num_cus\n",
    "    \n",
    "    def f32flops(self):\n",
    "        return self.fp32_flops * self.num_cus \n",
    "    \n",
    "    def transflops(self):\n",
    "        return self.trans_flops * self.num_cus\n",
    "    \n",
    "    def total_gpus(self):\n",
    "        return self.gpus_per_node * self.number_of_nodes\n",
    "    \n",
    "    def set_all2all(self,all2all_bwidth : dict):\n",
    "        self.all2all_bw = all2all_bwidth\n",
    "    \n",
    "    def get_all2all(self):\n",
    "        return self.all2all_bw\n",
    "    \n",
    "    def set_reduction(self,reduce_bwidth : dict):\n",
    "        self.reduction_bw= reduce_bwidth\n",
    "    \n",
    "    def get_reduction(self):\n",
    "        return self.reduction_bw\n",
    "    \n",
    "    def set_gather(self,gather_bwidth : dict):\n",
    "        self.gather_bw= gather_bwidth\n",
    "    \n",
    "    def get_gather(self):\n",
    "        return self.gather_bw\n",
    "    \n",
    "    def set_scatter(self,scatter_bwidth: dict):\n",
    "        self.scatter_bw= scatter_bwidth\n",
    "    \n",
    "    def get_scatter(self):\n",
    "        return self.scatter_bw\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class shardConfig:\n",
    "    dp_intra_degree : int =1\n",
    "    pp_intra_degree : int =1\n",
    "    tp_intra_degree : int =1\n",
    "    ep_intra_degree : int =1\n",
    "    #sp_intra_degree : int =1\n",
    "    \n",
    "    dp_inter_degree : int =1\n",
    "    pp_inter_degree : int =1\n",
    "    tp_inter_degree : int =1\n",
    "    ep_inter_degree : int =1\n",
    "    #sp_inter_degree : int =1 \n",
    "    \n",
    "    def tp_parallel_degree(self):\n",
    "        return self.tp_intra_degree * self.tp_inter_degree\n",
    "    def pp_parallel_degree(self):\n",
    "        return self.pp_intra_degree * self.pp_inter_degree\n",
    "    def dp_parallel_degree(self):\n",
    "        return self.dp_intra_degree * self.dp_inter_degree\n",
    "    def ep_parallel_degree(self):\n",
    "        return self.ep_intra_degree  * self.ep_inter_degree\n",
    "    #def sp_parallel_degree(self):\n",
    "    #    return self.sp_intra_degree * self.sp_inter_degree\n",
    "    \n",
    "    def parse_shard(self,_str):\n",
    "        shard_num = []\n",
    "        shard_str = []\n",
    "        substr_num = []\n",
    "        substr_char = []\n",
    "        for i in range(len(_str)):\n",
    "            if (_str[i].isnumeric()):\n",
    "                if(len(substr_char) == 2):\n",
    "                    shard_str.append(\"\".join(substr_char))\n",
    "                    substr_char.clear()\n",
    "                substr_num.append(_str[i])\n",
    "            elif(_str[i].isalpha()):\n",
    "                if (len(substr_num) >= 1):\n",
    "                    shard_num.append(int(\"\".join(substr_num)))\n",
    "                    substr_num.clear()\n",
    "                substr_char.append(_str[i])\n",
    "\n",
    "        if (len(substr_num)>=1):\n",
    "            shard_num.append(int(\"\".join(substr_num)))\n",
    "        if (len(substr_char) >=1):\n",
    "            shard_str.append(\"\".join(substr_char))\n",
    "        return(shard_str,tuple(shard_num))    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.dp_intra_degree =1\n",
    "        self.pp_intra_degree =1\n",
    "        self.tp_intra_degree =1\n",
    "        self.ep_intra_degree =1\n",
    "    #sp_intra_degree : int =1\n",
    "    \n",
    "        self.dp_inter_degree  =1\n",
    "        self.pp_inter_degree  =1\n",
    "        self.tp_inter_degree  =1\n",
    "        self.ep_inter_degree  =1\n",
    "    \n",
    "    def print_values(self):\n",
    "        print(f\" intra_cnfiguration ep={self.ep_intra_degree} dp={self.dp_intra_degree} tp={self.tp_intra_degree} pp={self.pp_intra_degree}\")\n",
    "        print(f\" inter_cnfiguration ep={self.ep_inter_degree} dp={self.dp_inter_degree} tp={self.tp_inter_degree} pp={self.pp_inter_degree}\")\n",
    "    \n",
    "    def setup_configuration(self, key: str, value: tuple):\n",
    "        (shard_key,shard_value) = self.parse_shard(key)\n",
    "        assert(shard_value == value)\n",
    "        #print(shard_key)\n",
    "        #print(shard_value)\n",
    "        for idx,item in enumerate(shard_key):\n",
    "            if (item == \"dp\"):\n",
    "                (self.dp_intra_degree, self.dp_inter_degree) = (1, shard_value[idx]) if shard_value[idx] <=8 else (shard_value[idx] // 8, 8)\n",
    "            elif(item == \"pp\"):\n",
    "                (self.pp_intra_degree, self.pp_inter_degree) = (1, shard_value[idx]) if shard_value[idx] <=8 else (shard_value[idx] // 8, 8)\n",
    "            elif(item == \"tp\"):\n",
    "                (self.tp_intra_degree, self.tp_inter_degree) = (shard_value[idx],1) if shard_value[idx] <=8 else (8,shard_value[idx] // 8)\n",
    "            elif(item == \"ep\"):\n",
    "                (self.ep_intra_degree, self.ep_inter_degree) = (1, shard_value[idx]) if shard_value[idx] <=8 else (shard_value[idx] // 8, 8)\n",
    "            else:\n",
    "                raise ValueError(f\" unsupported parallelism configuration given {item} supported = ep,tp,pp,dp\")\n",
    "                    \n",
    "@dataclass\n",
    "class layershard_configuration:\n",
    "    intra_degree : tuple = (1,1,1,1,1)   #dp,pp,tp,ep,sp\n",
    "    inter_degree : tuple = (1,1,1,1,1)\n",
    "    activation_einsum : str = None\n",
    "    weight_einsum : str = None\n",
    "    \n",
    "    def tp_parallel_degree(self):\n",
    "        return self.intra_degree[2] * self.inter_degree[2]\n",
    "    def pp_parallel_degree(self):\n",
    "        return self.intra_degree[1] * self.inter_degree[1]\n",
    "    def dp_parallel_degree(self):\n",
    "        return self.intra_degree[0] * self.inter_degree[0]\n",
    "    def ep_parallel_degree(self):\n",
    "        return self.intra_degree[0] * self.inter_degree[0]\n",
    "    def sp_parallel_degree(self):\n",
    "        return self.intra_degree[4] * self.inter_degree[4]\n",
    "    \n",
    "\n",
    "\n",
    "#batchsize configurations\n",
    "#batch_size = 32\n",
    "#minibatch_size = batch_size // dp_parallel_degree\n",
    "#microbatch_size = minibatch_size // pp_parallel_degree\n",
    "\n",
    "\n",
    "#inter & intra bw  #NV\n",
    "#network_card_bw = 200 ## Gbits/s\n",
    "#number_cards_per_node = 1 \n",
    "#network_card_latency = 100  #ns\n",
    "#intra_gpu_bw = 4.2 ## tbps/s\n",
    "#intra_network_latency = 100  #ns\n",
    "#total_intra_gpu_bw = intra_gpu_bw * gpus_per_node\n",
    "#total_inter_gpu_bw = network_card_bw * number_cards_per_node\n",
    "\n",
    "@dataclass\n",
    "class communication_config():\n",
    "    hidden_dim : int\n",
    "    context_len : int\n",
    "    batch_size : int\n",
    "    bpe: int\n",
    "    inter_bw: int\n",
    "    intra_bw: int\n",
    "    inter_nodes: int\n",
    "    intra_nodes: int\n",
    "    inter_latency : int\n",
    "    intra_latency : int\n",
    "    topology: str = \"ring\"\n",
    "\n",
    "    def payload(self):\n",
    "        return self.batch_size * self.hidden_dim * self.context_len * self.bpe\n",
    "    def payload_per_batch(self):\n",
    "        return self.hidden_dim * self.context_len * self.bpe\n",
    "\n",
    "@dataclass\n",
    "class algoConfig():\n",
    "    mode : str \n",
    "    qkv_gemm_eff : float\n",
    "    attn_gemm_eff : float\n",
    "    output_gemm_eff : float\n",
    "    moe_gemm_eff : float\n",
    "    ln_eff : float\n",
    "    rope_eff : float\n",
    "    topk_gating_eff : float\n",
    "    gelu_eff : float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_capacity(model_size: int, \n",
    "                 output_tokens: int,\n",
    "                 context_len : int,\n",
    "                 num_layers: int,\n",
    "                 hidden_dim: int,\n",
    "                 dtype: str = \"fp8\",\n",
    "                 batch_size: int = 16):\n",
    "\n",
    "   if dtype == \"fp8\":\n",
    "      parameter_byte = 1\n",
    "   elif dtype == \"fp4\":\n",
    "      parameter_byte = 0.5\n",
    "   else:\n",
    "      parameter_byte = 2\n",
    "\n",
    "   parameter_size = parameter_byte*(model_size)\n",
    "   kvcache_size = parameter_byte*batch_size * (context_len+output_tokens)*hidden_dim*num_layers*2    \n",
    "   total_size = kvcache_size + parameter_size\n",
    "   num_gpus = math.ceil(total_size / (8*24*1e9))\n",
    "   return num_gpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flops required for attention op  per layer\n",
    "\n",
    "#F_a  = 4*n**2  * d * h_q \n",
    "\n",
    "#R_a  = 2 * d * h_kv * l * n\n",
    "\n",
    "#Ai_a = 4 * n ** 2  * d  * h _q / 2 * d * h_kv * l * n \n",
    "#     = 2 * n * h_q / h_kv\n",
    "#     = n * h_q/h_kv\n",
    "     \n",
    "## AI of attention layer is directly proportional to context length (n)\n",
    "## for long context length inference, sharding tokens across all workers reduces AI\n",
    "## when number of tokens becomes too small , attention becomes communication/memory bound.\n",
    "\n",
    "## allocating more workers reduces latency and give better  TBT (time between token) but reduces hardware utilization\n",
    "\n",
    "#design constraint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intra payload \n",
    "\n",
    "def TP_communication(N:int, latency: float, BW_LINK: float, payload: int,topology: str=\"ring\"):\n",
    "    \"\"\"\n",
    "     N : for intra-node calculation , intra_node_parallelism degree \n",
    "     payload : activation payload ;  payload sliced based on (inter/intra) parallelism\n",
    "             : \n",
    "     BW_LINK : intra/inter link bandwidth \n",
    "             : intra : total_intra_bw // gpus_per_node\n",
    "             : inter :  total_inter_bw // gpus_per_node\n",
    "     latency : intra/inter link latency\n",
    "    \"\"\"\n",
    "    \n",
    "    factor = 1 \n",
    "    \n",
    "    if topology == \"ring\":\n",
    "        num_steps = factor*(N-1)/N\n",
    "    elif topology == \"Tree\":\n",
    "        num_steps = factor*math.log2(N)    \n",
    "    #network latency in (us)\n",
    "    #latency calculation in ms\n",
    "    latency_time = latency * 1e-3 * num_steps * N\n",
    "    payload_time =  (num_steps * payload * 1e-9 / (BW_LINK) ) * 1e3   #ms\n",
    "    print(f\"payload = {payload}\")\n",
    "    print(f\"TP payload_time {payload_time}\")\n",
    "    print(f\"TP latency time {latency_time}\")\n",
    "    return latency_time + payload_time\n",
    "        \n",
    "def PP_communication(latency: float, BW_LINK: float, payload: int):\n",
    "    \"\"\"\n",
    "    latency : inter/intra node latency\n",
    "    BW_link  : BW per GPU (divide up total/num_gpus_per_node)\n",
    "    \"\"\"\n",
    "    \n",
    "    payload_time = 1e3*((payload * 1e-9)/BW_LINK)\n",
    "    print(f\"payload = {payload}\")\n",
    "    print(f\"PP payload_time {payload_time}\")\n",
    "    print(f\"PP latency time {latency}\")\n",
    "    return ((latency * 1e-3) + payload_time)\n",
    "\n",
    "def EP_communication(payload: int, num_experts : int, total_gpus: int, \n",
    "                     gpus_per_node: int, latency_inter: int, BWIntra_LINK: int, BWInter_LINK: int):\n",
    "    #experts mapped to inter and intra nodes??\n",
    "    #tokens are euqally distributed to all number of expert nodes\n",
    "    # probability of token mapped to intra nodes  num_gpus/num_experts\n",
    "    \"\"\"\n",
    "     num_experts : total number of experts\n",
    "     num_gpus  : gpus per node\n",
    "     latency :  node -<>- node latency\n",
    "     BWIntra_LINK : bw of intra links \n",
    "     BWInter_LINK : bw if inter nodes links\n",
    "     payload : activation payload\n",
    "     bpe: bytes per element (activation precision)\n",
    "     \n",
    "    \"\"\"\n",
    "    compute_experts = min(num_experts,total_gpus)\n",
    "    \n",
    "    prob_intra_ep =  gpus_per_node / compute_experts\n",
    "    prob_inter_ep =  (1 - prob_intra_ep)\n",
    "    communication_steps = ( compute_experts - gpus_per_node)/compute_experts\n",
    "    #print(compute_experts,gpus_per_node)\n",
    "    #print(f\"communication_steps = {communication_steps}\")\n",
    "    #print(prob_intra_ep,prob_inter_ep)\n",
    "    #probility of token assigned to intra-node experts = num_gpus/num_experts\n",
    "    #probability of token assgined to inter-node experts = 1  - num_gpus/num_experts\n",
    "    payload_intra = payload * prob_intra_ep\n",
    "    payload_inter = payload * prob_inter_ep\n",
    "    print(payload_inter,payload_intra)\n",
    "    latency = 1e-3 * latency_inter * communication_steps * compute_experts / gpus_per_node  ## calculate actual number of GPUS actively involved in inter node communication\n",
    "    payload_latency = 1e3 * communication_steps * (1e-9 * ((payload_intra / BWIntra_LINK) + (payload_inter / BWInter_LINK)))\n",
    "    print(f\"EP payload_time {payload_latency}\")\n",
    "    print(f\"EP latency time {latency}\")\n",
    "    \n",
    "    return (latency + payload_latency)\n",
    "\n",
    "def pipeline_eff(pipeline_degree: int , num_micro_batches: int):\n",
    "    fraction_idle_time = (pipeline_degree-1)/num_micro_batches\n",
    "    pipeline_eff = (pipeline_degree -1) / (num_micro_batches+(pipeline_degree-1))\n",
    "    return pipeline_eff,fraction_idle_time\n",
    "\n",
    "\n",
    "def print_compute_str(profile_rec):\n",
    "    print(f\"qkv_time = {profile_rec['qkv_time']}\")\n",
    "    print(f\"attn_time = {profile_rec['attn_time']}\")\n",
    "    print(f\"outproj_time = {profile_rec['outproj_time']}\")\n",
    "    print(f\"mlp_time = {profile_rec['moe_time']}\")\n",
    "    print(f\"topk_softmax_time = {profile_rec['topk_softmax_time']}\")\n",
    "    print(f\"rope_time = {profile_rec['rope_time']}\")\n",
    "    print(f\"layernorm_time = {profile_rec['ln_time']}\")\n",
    "    \n",
    "\n",
    "def compute_forward_pass(mode: str, seqlen: int, algoCfg: algoConfig, modelCfg : modelConfig, hwCfg: GpuConfig, layer_dict : dict, verbose: bool = False):\n",
    "    if mode == \"TTFT\":   #prefill\n",
    "        #apply qkv_flops tp \n",
    "        qkv_time = modelCfg.qkv_flops(seqlen) * 1e-9 / (algoCfg.qkv_gemm_eff * hwCfg.f16flops() ) * (1/hwCfg.freq)  * 1e3\n",
    "        layer_dict[\"qkv_time\"] = qkv_time\n",
    "        attn_time = modelCfg.attn_flops(seqlen) * 1e-9 / (algoCfg.attn_gemm_eff * (hwCfg.f16flops())) * (1/hwCfg.freq) * 1e3\n",
    "        layer_dict[\"attn_time\"] = attn_time\n",
    "        out_time = modelCfg.outproj_flops(seqlen) * 1e-9 / (algoCfg.output_gemm_eff * (hwCfg.f16flops())) * (1/hwCfg.freq) *  1e3\n",
    "        layer_dict[\"outproj_time\"] = out_time\n",
    "        moe_time = modelCfg.moe_flops(seqlen) * 1e-9 / (algoCfg.moe_gemm_eff * hwCfg.f16flops()) *  (1/hwCfg.freq) * 1e3\n",
    "        gemm_cycles, softmax_cycles , topk_cycles = modelCfg.moe_topk_softmax(hwCfg.l2_latency,seqlen)\n",
    "        topk_softmax_moe= (1/hwCfg.freq) * 1e3 * ((gemm_cycles + softmax_cycles+topk_cycles) * 1e-9/(hwCfg.num_cus * 8))    ##occupancy 8\n",
    "        layer_dict[\"moe_time\"] = moe_time \n",
    "        layer_dict[\"topk_softmax_time\"] = topk_softmax_moe\n",
    "        hbm_bw_gfxclk = hwCfg.hbm_bw / hwCfg.freq \n",
    "        ln_time  = (modelCfg.layernorm_payload(seqlen) * 2) * 1e-9 / (algoCfg.ln_eff * hbm_bw_gfxclk)  * 1e3\n",
    "        layer_dict[\"ln_time\"] = ln_time\n",
    "        rope_time = (modelCfg.rope_payload(seqlen)*2) * 1e-9/ (algoCfg.rope_eff * hbm_bw_gfxclk) * 1e3\n",
    "        layer_dict[\"rope_time\"] = rope_time\n",
    "        compute_time= qkv_time+attn_time+out_time+moe_time+ln_time+rope_time+topk_softmax_moe\n",
    "        print(f\"forward pass time(ms)-prefill = {compute_time}\")\n",
    "        if (verbose):\n",
    "            print_compute_str(layer_dict)\n",
    "        return compute_time\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def fsdp_communication_overhead():\n",
    "    #retune the factor based on model size.\n",
    "    return 0.25\n",
    "\n",
    "def communication_tp_forward(payload:int,\n",
    "                             tp_intra_node:int, intra_latency: int, bw_intra:int,\n",
    "                             tp_inter_node:int, inter_latency: int, bw_inter:int):\n",
    "    ## tensor parallelism (inter and intra)\n",
    "    tp_intra_time = TP_communication(tp_intra_node,intra_latency,bw_intra,payload,'ring') if tp_intra_node > 1 else 0\n",
    "    tp_inter_time = TP_communication(tp_inter_node,inter_latency,bw_inter,payload,'ring') if tp_inter_node > 1 else 0\n",
    "    \n",
    "    return tp_inter_time + tp_intra_time\n",
    "\n",
    "def communication_pp_forward(payload:int, latency: int, bw_inter: int , inter_nodes:int, bw_intra:int, intra_nodes:int):\n",
    "\n",
    "\n",
    "    ## layer sliced across nodes so need to divide this by layers so multiplying by laers\n",
    "    pp_inter_time = PP_communication(latency,bw_inter,payload) if inter_nodes > 1 else 0\n",
    "    pp_intra_time = PP_communication(latency,bw_intra,payload) if intra_nodes > 1 else 0\n",
    "    \n",
    "    return max(pp_inter_time,pp_intra_time)\n",
    "\n",
    "\n",
    "def communication_forward_pass(shardCfg: shardConfig, \n",
    "                               comCfg: communication_config, \n",
    "                               gpus_per_node:int , num_nodes:int,\n",
    "                               modelCfg: modelConfig):\n",
    "    \n",
    "    assert(comCfg != None)     \n",
    "    #fixed PP sharding config.\n",
    "    \n",
    "    gpus_in_config = shardCfg.dp_parallel_degree() * shardCfg.tp_parallel_degree() * shardCfg.pp_parallel_degree()\n",
    "    nodes_in_config = gpus_in_config // (shardCfg.pp_intra_degree * shardCfg.tp_intra_degree * shardCfg.dp_intra_degree)\n",
    "    gpus_in_intra = gpus_in_config//nodes_in_config\n",
    "    \n",
    "    pp_inter_parallelism = shardCfg.pp_inter_degree\n",
    "    pp_intra_parallelsim = shardCfg.pp_intra_degree\n",
    "    \n",
    "    \n",
    "    bw_inter = comCfg.inter_bw // gpus_in_intra\n",
    "    bw_intra = comCfg.intra_bw \n",
    "    payload = comCfg.context_len * comCfg.hidden_dim * comCfg.bpe\n",
    "    pp_communication_time =  communication_pp_forward(payload,\n",
    "                                                      comCfg.inter_latency,\n",
    "                                                      bw_inter,\n",
    "                                                      pp_inter_parallelism,\n",
    "                                                      bw_intra,\n",
    "                                                      pp_intra_parallelsim)\n",
    "    \n",
    "    pp_communication_time = pp_communication_time//modelCfg.num_layers   # total time is multipled by layers * batch_size\n",
    "    \n",
    "    \n",
    "    #Add per layer sharding configuration\n",
    "    # QKV math split Weight[h,3h]dimension between intra & inter  h/intra, 3h/inter  \n",
    "    # attention layer  BS * heads -> intra\n",
    "    # output projection     \n",
    "    \n",
    "    bw_intra = comCfg.intra_bw\n",
    "    tp_intra_time = TP_communication(shardCfg.tp_intra_degree,\n",
    "                                     comCfg.intra_latency,bw_intra,payload//nodes_in_config,'ring') if shardCfg.tp_intra_degree > 1 else 0\n",
    "\n",
    "\n",
    "    bw_inter = comCfg.inter_bw // (gpus_in_intra)\n",
    "    tp_inter_time = TP_communication(shardCfg.tp_inter_degree,\n",
    "                                     comCfg.inter_latency,bw_inter,(payload//gpus_in_intra),'ring') if  shardCfg.tp_inter_degree > 1 else 0\n",
    "    \n",
    "    tp_communication_time = tp_intra_time + tp_inter_time\n",
    "    \n",
    "    ep_communication_time = 0\n",
    "    bw_intra = comCfg.intra_bw\n",
    "    bw_inter = comCfg.inter_bw\n",
    "    if (shardCfg.ep_inter_degree > 1 or shardCfg.ep_intra_degree > 1):\n",
    "         ep_communication_time = EP_communication(payload,\n",
    "                                                  modelCfg.total_experts,\n",
    "                                                  gpus_in_config,\n",
    "                                                  gpus_in_intra,\n",
    "                                                  comCfg.inter_latency,\n",
    "                                                  bw_intra,\n",
    "                                                  bw_inter)\n",
    "                                                  \n",
    "    ## 2 tp , ep per layer\n",
    "    forward_com_time = pp_communication_time + 2*tp_communication_time + 2*ep_communication_time  \n",
    "    forward_com_time = (1 + fsdp_communication_overhead()) * forward_com_time\n",
    "    return forward_com_time\n",
    "\n",
    "def pipeline_bubble_time_forward(shardCfg: shardConfig,\n",
    "                                 comCfg: communication_config,\n",
    "                                 compute_time : float,\n",
    "                                 hwCfg : GpuConfig,\n",
    "                                 modelCfg : modelConfig\n",
    "                                 ):\n",
    "    ## n workers m micro_batches  idle slots = n-1 total slots = n+m-1  per pass\n",
    "    ## bubble fraction time = n-1/ (m+n-1)\n",
    "    \n",
    "    ## n layers per model are equally distributed for pp_parallel_degree ; divide by num_layers for later calculations\n",
    "    num_microbatches = modelCfg.num_microbatches\n",
    "    #compute_time =  compute_forward_pass(seqlen,algoCfg,modelCfg,hwCfg)//(total_gpus * num_layers)  \n",
    "    comtime = communication_forward_pass(shardCfg,comCfg,hwCfg.gpus_per_node,hwCfg.number_of_nodes,modelCfg)\n",
    "    bubble_time = 1/num_microbatches * (shardCfg.pp_parallel_degree()-1) * (compute_time + comtime)\n",
    "    return bubble_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mi300 configuration\n",
    "hw_dict = {\n",
    "        \"mi300\" : {\"fp16_flops\" : 2048, \"fp32_flops\" : 128, \"fma_flops\" : 256 , \"freq\" : 1.3 , \"hbm_bw\" : 4200, \"num_cus\" : 308, \"trans_flops\" : 16,\n",
    "                    \"hbm_capacity\" : 160 , \"gpus_per_node\" : 8 , \"number_of_nodes\" : 8, \"topology\" : \"ring\", \"intracommEff\" : .7, \"intercommEff\" : .5, \"l2_latency\" : 320, \"hbm_latency\" : 640,\n",
    "                    \"intra_bw\" : 7*64 , \"nic_cards\" : 8, \"inter_bw\" : 50 , \"intra_latency\" : 1 , \"inter_latency\" : 2 , \"gather_bw\" : {},\n",
    "                    \"scatter_bw\" : {}, \"all2all_bw\" : {}, \"reduction_bw\" : {}},\n",
    " }\n",
    "\n",
    "algo_dict = {\n",
    "        \n",
    "        \"ttft\" : {\"qkv_gemm_eff\" : .85 , \"attn_gemm_eff\" : .8 , \"output_gemm_eff\" : .85 , \"moe_gemm_eff\" : .75 , \"ln_eff\" : .65,\n",
    "         \"rope_eff\" : .8 , \"topk_gating_eff\" : .6, \"gelu_eff\" : .7 },\n",
    "        \"tpot\" : {},\n",
    "}\n",
    "\n",
    "prefillCfg = algo_dict[\"ttft\"]\n",
    "\n",
    "class shardConfig:\n",
    "    dp_intra_degree : int =1\n",
    "    pp_intra_degree : int =1\n",
    "    tp_intra_degree : int =1\n",
    "    ep_intra_degree : int =1\n",
    "    sp_intra_degree : int =1\n",
    "    \n",
    "    dp_inter_degree : int =1\n",
    "    pp_inter_degree : int =1\n",
    "    tp_inter_degree : int =1\n",
    "    ep_inter_degree : int =1\n",
    "    sp_inter_degree : int =1 \n",
    "\n",
    "tensor_parallel = [1,2,4,8,16,32,64]\n",
    "pipeline_parallel = [1,2,4,8,16,32,64]\n",
    "data_parallel = [1,2,4]\n",
    "expert_parallel = [1,2,4,8,16]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build shard connfigurations. \n",
    "#num gpus = 64\n",
    "\n",
    "tensor_parallel = [1,2,4,8,16,32,64]\n",
    "pipeline_parallel = [1,2,4,8,16,32,64]\n",
    "data_parallel = [1,2,4]\n",
    "expert_parallel = [1,2,4,8,16]\n",
    "\n",
    "\n",
    "def return_key(inp: tuple):\n",
    "    dp,pp,tp,ep = inp\n",
    "    val = ()\n",
    "    temp_list = list(val)\n",
    "    key_str = \" \"\n",
    "    if (tp > 1):\n",
    "        key_str += \"tp\" + f\"{tp}\"\n",
    "        temp_list.append(tp)\n",
    "    if (pp > 1):\n",
    "        key_str += \"pp\" + f\"{pp}\"\n",
    "        temp_list.append(pp)\n",
    "    if (dp > 1):\n",
    "        key_str += \"dp\" + f\"{dp}\"\n",
    "        temp_list.append(dp)\n",
    "    if (ep > 1):\n",
    "        key_str += \"ep\" + f\"{ep}\"\n",
    "        temp_list.append(ep)\n",
    "        \n",
    "    val = tuple(temp_list)\n",
    "    return(key_str,val)   \n",
    "    \n",
    "permute_list = [(dp,pp,tp,ep) for dp in tensor_parallel for pp in pipeline_parallel for tp in data_parallel for ep in expert_parallel]\n",
    "#print(len(permute_list))\n",
    "\n",
    "shard_permute = {}\n",
    "num_gpus = 64\n",
    "for item in permute_list:\n",
    "    dp,pp,tp,ep = item\n",
    "    if (tp*pp*dp*ep == num_gpus):\n",
    "        (key_str,value) = return_key(item)\n",
    "        shard_permute[key_str] = value\n",
    "\n",
    "#remove one configuration from the dict\n",
    "shard_cfg = {key: value for key,value in shard_permute.items() if len(value) > 1}\n",
    "#print(len(shard_cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttft_time(batch_size:int, seqlen:int):\n",
    "    modelCfg = modelConfig()\n",
    "    hwCfg = GpuConfig(**hw_dict[\"mi300\"])\n",
    "    ttftCfg = algoConfig(\"ttft\",**prefillCfg)\n",
    "    shardCfg  = shardConfig()\n",
    "\n",
    "    ttft_dict = {}\n",
    "    layercompute_dict = {\"qkv_time\":0, \"attn_time\" : 0 , \"outproj_time\" : 0, \"moe_time\" : 0, \"ln_time\" : 0 , \"rope_time\" : 0 }\n",
    "    #computetime_spent = compute_forward_pass(\"TTFT\",seqlen,ttftCfg,modelCfg,hwCfg,layercompute_dict)\n",
    "    comCfg = communication_config(modelCfg.hidden_dim,\n",
    "                                  seqlen,\n",
    "                                  batch_size,\n",
    "                                  modelCfg.wt_bpe,\n",
    "                                  hwCfg.nic_cards * hwCfg.inter_bw * hwCfg.intercommEff,\n",
    "                                  hwCfg.intra_bw * hwCfg.intracommEff,\n",
    "                                  hwCfg.number_of_nodes,\n",
    "                                  hwCfg.gpus_per_node,\n",
    "                                  hwCfg.inter_latency,\n",
    "                                  hwCfg.intra_latency,\"ring\")\n",
    "    computetime_spent = compute_forward_pass(\"TTFT\",seqlen,ttftCfg,modelCfg,hwCfg,layercompute_dict,True)\n",
    "    for (key,val) in shard_cfg.items():\n",
    "         shardCfg.setup_configuration(key,val)\n",
    "         print(f\"shard_topology : {key}\")\n",
    "         modelCfg.num_microbatches = shardCfg.pp_inter_degree * shardCfg.pp_intra_degree\n",
    "         commtime_spent = communication_forward_pass(shardCfg,comCfg,hwCfg.gpus_per_node,hwCfg.number_of_nodes,modelCfg)\n",
    "         parallelism_degree = shardCfg.pp_parallel_degree() * shardCfg.tp_parallel_degree() * shardCfg.dp_parallel_degree()\n",
    "         computetime = computetime_spent//(parallelism_degree * modelCfg.num_layers)\n",
    "         pipeline_time = pipeline_bubble_time_forward(shardCfg,comCfg,computetime,hwCfg,modelCfg) if (shardCfg.pp_inter_degree * shardCfg.pp_intra_degree) > 1 else 0\n",
    "         time_batch_size_layer = (computetime_spent//(parallelism_degree) + commtime_spent + pipeline_time)\n",
    "         print(time_batch_size_layer)\n",
    "         time = batch_size * modelCfg.num_layers * time_batch_size_layer\n",
    "         ttft_dict[key] = { key : shard_cfg[key], \"latency\" : time}\n",
    "         shardCfg.reset()\n",
    "         assert(0)\n",
    "    return ttft_dict\n",
    "    #for (key,val) in shard_cfg.items():\n",
    "    #    shardCfg.setup_configuration(key,val)\n",
    "    #    #shardCfg.print_values()\n",
    "    #    shardCfg.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass time(ms)-prefill = 52.28390700194937\n",
      "qkv_time = 16.304472562731384\n",
      "attn_time = 17.59847832167832\n",
      "outproj_time = 5.434824187577129\n",
      "mlp_time = 12.318934825174825\n",
      "topk_softmax_time = 0.019023024787712287\n",
      "rope_time = 0.27262976000000005\n",
      "layernorm_time = 0.33554432\n",
      "shard_topology :  tp4ep16\n",
      "payload = 352321536\n",
      "TP payload_time 0.8426057142857145\n",
      "TP latency time 0.003\n",
      "4 4\n",
      "communication_steps = 0.0\n",
      "1.0 0.0\n",
      "0.0 352321536.0\n",
      "EP payload_time 0.0\n",
      "EP latency time 0.0\n",
      "15.114014285714287\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mttft_time\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m16384\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[78], line 34\u001b[0m, in \u001b[0;36mttft_time\u001b[1;34m(batch_size, seqlen)\u001b[0m\n\u001b[0;32m     32\u001b[0m      ttft_dict[key] \u001b[38;5;241m=\u001b[39m { key : shard_cfg[key], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatency\u001b[39m\u001b[38;5;124m\"\u001b[39m : time}\n\u001b[0;32m     33\u001b[0m      shardCfg\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 34\u001b[0m      \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ttft_dict\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ttft_time(16,16384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttft_time(batch_size:int, seqlen:int):\n",
    "    modelCfg = modelConfig()\n",
    "    hwCfg = GpuConfig(**hw_dict[\"mi300\"])\n",
    "    ttftCfg = algoConfig(\"ttft\",**prefillCfg)\n",
    "    shardCfg  = shardConfig()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
